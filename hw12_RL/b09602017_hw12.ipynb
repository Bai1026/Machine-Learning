{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp30SB4bxeQb"
      },
      "source": [
        "# **Homework 12 - Reinforcement Learning**\n",
        "\n",
        "If you have any problem, e-mail us at mlta-2023-spring@googlegroups.com\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yXsnCWPtWSNk"
      },
      "source": [
        "## Preliminary work\n",
        "\n",
        "First, we need to install all necessary packages.\n",
        "One of them, gym, builded by OpenAI, is a toolkit for developing Reinforcement Learning algorithm. Other packages are for visualization in colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e2bScpnkVbv",
        "outputId": "6ada0823-e8e7-4671-e3de-b12037d86b1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\u001b[0m\n",
            "\u001b[1;31mE: \u001b[0mUnable to lock directory /var/lib/apt/lists/\u001b[0m\n",
            "\u001b[1;33mW: \u001b[0mProblem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\u001b[0m\n",
            "\u001b[1;33mW: \u001b[0mProblem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\u001b[0m\n",
            "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
            "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n",
            "Requirement already satisfied: box2d==2.3.2 in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (2.3.2)\n",
            "Requirement already satisfied: gym[box2d]==0.25.2 in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (0.25.2)\n",
            "Requirement already satisfied: box2d-py in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (2.3.5)\n",
            "Requirement already satisfied: pyvirtualdisplay in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (3.0)\n",
            "Requirement already satisfied: tqdm in /home/wmnlab/.local/lib/python3.10/site-packages (4.65.0)\n",
            "Requirement already satisfied: numpy==1.22.4 in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (from gym[box2d]==0.25.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (from gym[box2d]==0.25.2) (0.0.8)\n",
            "Requirement already satisfied: swig==4.* in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (from gym[box2d]==0.25.2) (4.1.1)\n",
            "Requirement already satisfied: pygame==2.1.0 in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (from gym[box2d]==0.25.2) (2.1.0)\n",
            "Requirement already satisfied: box2d==2.3.2 in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (2.3.2)\n",
            "Requirement already satisfied: box2d-kengz in /home/wmnlab/anaconda3/envs/sheng-ru/lib/python3.10/site-packages (2.3.3)\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install python-opengl xvfb -y\n",
        "!pip install -q swig\n",
        "!pip install box2d==2.3.2 gym[box2d]==0.25.2 box2d-py pyvirtualdisplay tqdm numpy==1.22.4 \n",
        "!pip install box2d==2.3.2 box2d-kengz\n",
        "!pip freeze > requirements.txt\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M_-i3cdoYsks"
      },
      "source": [
        "\n",
        "Next, set up virtual display，and import all necessaary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nl2nREINDLiw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CaEJ8BUCpN9P"
      },
      "source": [
        "# Warning ! Do not revise random seed !!!\n",
        "# Your submission on JudgeBoi will not reproduce your result !!!\n",
        "Make your HW result to be reproducible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fV9i8i2YkRbO"
      },
      "outputs": [],
      "source": [
        "seed = 2023 # Do not change this\n",
        "def fix(env, seed):\n",
        "  env.seed(seed)\n",
        "  env.action_space.seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "He0XDx6bzjgC"
      },
      "source": [
        "Last, call gym and build an [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N_4-xJcbBt09"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import gym\n",
        "import random\n",
        "env = gym.make('LunarLander-v2')\n",
        "fix(env, seed) # fix the environment Do not revise this !!!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkVvTrvWZ5H"
      },
      "source": [
        "## What Lunar Lander？\n",
        "\n",
        "“LunarLander-v2”is to simulate the situation when the craft lands on the surface of the moon.\n",
        "\n",
        "This task is to enable the craft to land \"safely\" at the pad between the two yellow flags.\n",
        "> Landing pad is always at coordinates (0,0).\n",
        "> Coordinates are the first two numbers in state vector.\n",
        "\n",
        "![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n",
        "\n",
        "\"LunarLander-v2\" actually includes \"Agent\" and \"Environment\". \n",
        "\n",
        "In this homework, we will utilize the function `step()` to control the action of \"Agent\". \n",
        "\n",
        "Then `step()` will return the observation/state and reward given by the \"Environment\"."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bIbp82sljvAt"
      },
      "source": [
        "### Observation / State\n",
        "\n",
        "First, we can take a look at what an Observation / State looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsXZra3N9R5T",
        "outputId": "4fc632a8-4e4d-42ac-c070-0c170f6d5342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
            " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
            " 1.       ], (8,), float32)\n"
          ]
        }
      ],
      "source": [
        "print(env.observation_space)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdfoThbAQ49"
      },
      "source": [
        "\n",
        "`Box(8,)`means that observation is an 8-dim vector\n",
        "### Action\n",
        "\n",
        "Actions can be taken by looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1k4dIrBAaKi",
        "outputId": "fd702411-c918-4a31-c689-d758d4bd0007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ],
      "source": [
        "print(env.action_space)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dejXT6PHBrPn"
      },
      "source": [
        "`Discrete(4)` implies that there are four kinds of actions can be taken by agent.\n",
        "- 0 implies the agent will not take any actions\n",
        "- 2 implies the agent will accelerate downward\n",
        "- 1, 3 implies the agent will accelerate left and right\n",
        "\n",
        "Next, we will try to make the agent interact with the environment. \n",
        "Before taking any actions, we recommend to call `reset()` function to reset the environment. Also, this function will return the initial state of the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi4OmrmZgnWA",
        "outputId": "b884c29d-20b1-4375-fe79-5fbcd1f7fbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.00506535  1.413064   -0.5130838   0.09527162  0.00587628  0.11622101\n",
            "  0.          0.        ]\n"
          ]
        }
      ],
      "source": [
        "initial_state = env.reset()\n",
        "print(initial_state)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uBx0mEqqgxJ9"
      },
      "source": [
        "Then, we try to get a random action from the agent's action space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxkOEXRKgizt",
        "outputId": "ec201c27-6af9-48fe-dece-5e0772d0315e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "random_action = env.action_space.sample()\n",
        "print(random_action)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mns-bO01g0-J"
      },
      "source": [
        "More, we can utilize `step()` to make agent act according to the randomly-selected `random_action`.\n",
        "The `step()` function will return four values:\n",
        "- observation / state\n",
        "- reward\n",
        "- done (True/ False)\n",
        "- Other information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E_WViSxGgIk9"
      },
      "outputs": [],
      "source": [
        "observation, reward, done, info = env.step(random_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK7r126kuCNp",
        "outputId": "a377b67b-7668-4305-fdbd-9d0005d1419e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(done)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdS8vOihxhc"
      },
      "source": [
        "### Reward\n",
        "\n",
        "\n",
        "> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxQNs77hi0_7",
        "outputId": "07ebc1e2-a9b9-442b-bf7b-9a5f7bad958e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1.4981841929643156\n"
          ]
        }
      ],
      "source": [
        "print(reward)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhqp6D-XgHpe"
      },
      "source": [
        "### Random Agent\n",
        "In the end, before we start training, we can see whether a random agent can successfully land the moon or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "Y3G0bxoccelv",
        "outputId": "ed047422-6cd6-4adc-8f24-895f29f265dd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAF7CAYAAAD4/3BBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+AElEQVR4nO3de1yUdd4//tccmOE4M5yGATmIiCgqiKg4mmZKHvJQ5m5q/oz19s7Nxb6ZbbdLd9m2J9ra3WrvbW1/3W2HzUOHb7ZlHiINzEQlFU8ICpKgMIAgM4AyHObz/YO4csoSEJ1r4PV8PD45M9eH63pfH4h5cc11fS6FEEKAiIiISEaUri6AiIiI6LsYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHZcGlBefvllDBw4EJ6enkhJScHBgwddWQ4RERHJhMsCyjvvvIM1a9bg6aefxuHDh5GYmIgZM2agurraVSURERGRTChcdbPAlJQUjB07Fn/7298AAA6HAxEREXj44Yfxq1/9yhUlERERkUyoXbHRlpYWHDp0CBkZGdJrSqUSqampyM3N/V5/u90Ou90uPXc4HKirq0NgYCAUCsUtqZmIiIhujBACDQ0NCAsLg1L54x/iuCSgXLx4Ee3t7QgJCXF6PSQkBIWFhd/rn5mZiWeeeeZWlUdEREQ3UXl5OcLDw3+0j1tcxZORkQGr1Sq1srIyV5dEREREPeTn53fdPi45ghIUFASVSoWqqiqn16uqqmAymb7XX6vVQqvV3qryiIiI6CbqyukZLjmCotFokJycjF27dkmvORwO7Nq1C2az2RUlERERkYy45AgKAKxZswZpaWkYM2YMxo0bhxdffBFNTU1YtmyZq0oiIiIimXBZQFm4cCFqamqwbt06WCwWjBo1Cjt27PjeibNERETU/7hsHpQbYbPZoNfrXV0GERER9YDVaoVOp/vRPm5xFQ8RERH1LwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7vR5Qfv3rX0OhUDi1oUOHSsubm5uRnp6OwMBA+Pr6YsGCBaiqqurtMoiIiMiN3ZQjKMOHD0dlZaXU9u7dKy179NFH8fHHH+O9995DTk4OKioqcO+9996MMoiIiMhNqW/KStVqmEym771utVrx2muvYePGjZg6dSoA4PXXX8ewYcOwf/9+jB8//maUQ0RERG7mphxBOXPmDMLCwjBo0CAsWbIEZWVlAIBDhw6htbUVqampUt+hQ4ciMjISubm5P7g+u90Om83m1IiIiKjv6vWAkpKSgjfeeAM7duzA+vXrUVpaikmTJqGhoQEWiwUajQYGg8Hpa0JCQmCxWH5wnZmZmdDr9VKLiIjo7bKJiIhIRnr9I55Zs2ZJjxMSEpCSkoKoqCi8++678PLy6tE6MzIysGbNGum5zWZjSCEiIurDbvplxgaDAUOGDEFxcTFMJhNaWlpQX1/v1Keqquqa56x00mq10Ol0To2IiIj6rpseUBobG1FSUoLQ0FAkJyfDw8MDu3btkpYXFRWhrKwMZrP5ZpdCREREbqLXP+L55S9/iblz5yIqKgoVFRV4+umnoVKpsHjxYuj1eixfvhxr1qxBQEAAdDodHn74YZjNZl7BQ0RERJJeDyjnz5/H4sWLUVtbi+DgYNx2223Yv38/goODAQAvvPAClEolFixYALvdjhkzZuDvf/97b5dBREREbkwhhBCuLqK7bDYb9Hq9q8sgIiKiHrBardc9n5T34iEiIiLZYUAhIiIi2WFAISIiItlhQCEiIiLZYUAhIiIi2WFAISIiItlhQCEiIiLZYUAhIiIi2WFAISIiItlhQCEiIiLZYUAhIiIi2WFAISIiItlhQCEiIiLZYUAhIiIi2WFAISIiItlhQCEiIiLZYUAhIiIi2WFAISIiItlhQCEiIiLZYUAhIiIi2WFAISIiItlhQCEiIiLZYUAhIiIi2WFAISIiItlhQCEiIiLZYUAhIiIi2WFAISIiItlhQCEiIiLZYUAhIiIi2WFAISIiItlhQCEiIiLZ6XZA2bNnD+bOnYuwsDAoFAp8+OGHTsuFEFi3bh1CQ0Ph5eWF1NRUnDlzxqlPXV0dlixZAp1OB4PBgOXLl6OxsfGGdoSIiIj6jm4HlKamJiQmJuLll1++5vLnnnsOf/3rX/HKK6/gwIED8PHxwYwZM9Dc3Cz1WbJkCU6ePImsrCxs3boVe/bswYoVK3q+F0RERNS3iBsAQGzZskV67nA4hMlkEs8//7z0Wn19vdBqtWLTpk1CCCEKCgoEAJGXlyf12b59u1AoFOLChQtd2q7VahUA2NjY2NjY2NywWa3W677X9+o5KKWlpbBYLEhNTZVe0+v1SElJQW5uLgAgNzcXBoMBY8aMkfqkpqZCqVTiwIED11yv3W6HzWZzakRERNR39WpAsVgsAICQkBCn10NCQqRlFosFRqPRablarUZAQIDU57syMzOh1+ulFhER0ZtlExERkcy4xVU8GRkZsFqtUisvL3d1SURERHQT9WpAMZlMAICqqiqn16uqqqRlJpMJ1dXVTsvb2tpQV1cn9fkurVYLnU7n1IiIiKjv6tWAEh0dDZPJhF27dkmv2Ww2HDhwAGazGQBgNptRX1+PQ4cOSX12794Nh8OBlJSU3iyHiIiI3JS6u1/Q2NiI4uJi6XlpaSny8/MREBCAyMhIrF69Gr/73e8QGxuL6OhoPPXUUwgLC8M999wDABg2bBhmzpyJBx98EK+88gpaW1uxatUqLFq0CGFhYb22Y0REROTGunhFseTzzz+/5iVDaWlpQoiOS42feuopERISIrRarZg2bZooKipyWkdtba1YvHix8PX1FTqdTixbtkw0NDR0uQZeZszGxsbGxua+rSuXGSuEEAJuxmazQa/Xu7oMIiIi6gGr1Xrd80nd4ioeIiIi6l8YUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdrodUPbs2YO5c+ciLCwMCoUCH374odPyn/3sZ1AoFE5t5syZTn3q6uqwZMkS6HQ6GAwGLF++HI2NjTe0I0RERNR3dDugNDU1ITExES+//PIP9pk5cyYqKyultmnTJqflS5YswcmTJ5GVlYWtW7diz549WLFiRferJyIior5J3AAAYsuWLU6vpaWlibvvvvsHv6agoEAAEHl5edJr27dvFwqFQly4cKFL27VarQIAGxsbGxsbmxs2q9V63ff6m3IOSnZ2NoxGI+Li4rBy5UrU1tZKy3Jzc2EwGDBmzBjptdTUVCiVShw4cOCa67Pb7bDZbE6NiIiI+q5eDygzZ87EW2+9hV27duGPf/wjcnJyMGvWLLS3twMALBYLjEaj09eo1WoEBATAYrFcc52ZmZnQ6/VSi4iI6O2yiYiISEbUvb3CRYsWSY9HjhyJhIQExMTEIDs7G9OmTevROjMyMrBmzRrpuc1mY0ghIiLqw276ZcaDBg1CUFAQiouLAQAmkwnV1dVOfdra2lBXVweTyXTNdWi1Wuh0OqdGREREfddNDyjnz59HbW0tQkNDAQBmsxn19fU4dOiQ1Gf37t1wOBxISUm52eUQERGRG+j2RzyNjY3S0RAAKC0tRX5+PgICAhAQEIBnnnkGCxYsgMlkQklJCf7rv/4LgwcPxowZMwAAw4YNw8yZM/Hggw/ilVdeQWtrK1atWoVFixYhLCys9/aMiIiI3FeXruu9yueff37NS4bS0tLE5cuXxfTp00VwcLDw8PAQUVFR4sEHHxQWi8VpHbW1tWLx4sXC19dX6HQ6sWzZMtHQ0NDlGniZMRsbGxsbm/u2rlxmrBBCCLgZm80GvV7v6jKIiIioB6xW63XPJ+W9eIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHa6FVAyMzMxduxY+Pn5wWg04p577kFRUZFTn+bmZqSnpyMwMBC+vr5YsGABqqqqnPqUlZVh9uzZ8Pb2htFoxOOPP462trYb3xsiIiLqE7oVUHJycpCeno79+/cjKysLra2tmD59OpqamqQ+jz76KD7++GO89957yMnJQUVFBe69915peXt7O2bPno2Wlhbs27cPb775Jt544w2sW7eu9/aKiIiI3Ju4AdXV1QKAyMnJEUIIUV9fLzw8PMR7770n9Tl16pQAIHJzc4UQQmzbtk0olUphsVikPuvXrxc6nU7Y7fYubddqtQoAbGxsbGxsbG7YrFbrdd/rb+gcFKvVCgAICAgAABw6dAitra1ITU2V+gwdOhSRkZHIzc0FAOTm5mLkyJEICQmR+syYMQM2mw0nT5685nbsdjtsNptTIyIior6rxwHF4XBg9erVmDhxIkaMGAEAsFgs0Gg0MBgMTn1DQkJgsVikPleHk87lncuuJTMzE3q9XmoRERE9LZuIiIjcQI8DSnp6Ok6cOIHNmzf3Zj3XlJGRAavVKrXy8vKbvk0iIiJyHXVPvmjVqlXYunUr9uzZg/DwcOl1k8mElpYW1NfXOx1FqaqqgslkkvocPHjQaX2dV/l09vkurVYLrVbbk1KJiIjIDXXrCIoQAqtWrcKWLVuwe/duREdHOy1PTk6Gh4cHdu3aJb1WVFSEsrIymM1mAIDZbMbx48dRXV0t9cnKyoJOp0N8fPyN7AsRERH1Fd24aEesXLlS6PV6kZ2dLSorK6V2+fJlqc9DDz0kIiMjxe7du8VXX30lzGazMJvN0vK2tjYxYsQIMX36dJGfny927NghgoODRUZGRpfr4FU8bGxsbGxs7tu6chVPtwLKD23o9ddfl/pcuXJF/OIXvxD+/v7C29tbzJ8/X1RWVjqt5+uvvxazZs0SXl5eIigoSDz22GOitbW1y3UwoLCxsbGxsblv60pAUXwTPNyKzWaDXq93dRlERETUA1arFTqd7kf78F48REREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREXfD322/HS5MmubqMfkPt6gKIiIjk7h933IFlw4ZBCAEBYPUXX7i6pD6PAYWIiOg6VAoFFFc9ppuPAYWIiOg6/nP3bggh0Opw4OE9e1xdTr+gEEIIVxfRXTabDXq93tVlEBERUQ9YrVbodLof7cOTZImIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiIeplarYVarXV1GW6NAYWIiKgXaTQ+GDd2Cfz8QgBwUree4kRtREREvcTPNwTmscsQa5oOH00wTpd+jorK42htveLq0twOAwoREVEvCAqMwR3jH8Ug4xR4eQTAMDgS0aG3Yf+JV3Gu/CtYrRWuLtGtcCZZIiKiGxQUMAi3pzyCGNNUeKkDoPjmfj1COFDdcArlFw/g2OkPcOHCCbS3t7i4WtfrykyyPIJCRER0A/x8Q3CHeQ2ijbfDS+0vhRMAUCiUMPrFw+AdCX+/gfjK502Ulh5Ac7PNhRW7Bx5BISIi6iFPTz0WzPoLwg3j4ak2OIWTqwkhIOCAtbkchRc+Ru5Xr6OhoRpCOG5xxfLAIyhEREQ9pgDww3/D63VhuGvqrxHub4aX2vDja1IooIAK/l4DMXpgGgL0A3Ho+GacK8+D3d7Yq1X3FQwoRERE3xEUNAhCOFBb+/W1lwfGYErKI4j0nwhPVfeO6GvVOgwJmgP92EicCPw3Cs5sR339eTgc7b1Qed/RrXlQMjMzMXbsWPj5+cFoNOKee+5BUVGRU58pU6Z0JMWr2kMPPeTUp6ysDLNnz4a3tzeMRiMef/xxtLW13fjeEBER3QCNxhsxMRMxccwKxMbefs0+Af4Dcfu4/4OYkKk/+rHOj1EoFAjxHYnxQ1fAnLQckRGjoVTymMHVujUaOTk5SE9Px9ixY9HW1oYnnngC06dPR0FBAXx8fKR+Dz74IH7zm99Iz729vaXH7e3tmD17NkwmE/bt24fKyko88MAD8PDwwB/+8Ide2CUiIqLuMxjCMWrEfMQOmI4gnyHQeYfjXOhXqKw8KfXx8Q7AtIm/xKDgKT0OJ50UCiV8NCFIiFoEU8BInDB+gPxjW/iRzzdu6CTZmpoaGI1G5OTkYPLkyQA6jqCMGjUKL7744jW/Zvv27ZgzZw4qKioQEhICAHjllVewdu1a1NTUQKPRXHe7PEmW6NqeeAK46y5ACKC5GXjnHWDLlo5lQgB2O9DU5Noa+4vZs4GMjI5xb2sDcnOBP//52+WtrYCNF3K4nEKhgIeHN4YPnYWhMdMRYTDDS+0PQAF7uw2Hvv4nDuW/g7q6Mnh5GnDXneswKHAavD2CbiicfJcQDlxuqcWhr19H4eksWKoK+/RHPjf9JFmr1QoACAgIcHp9w4YNePvtt2EymTB37lw89dRT0lGU3NxcjBw5UgonADBjxgysXLkSJ0+eRFJS0ve2Y7fbYbfbpec2/l9NdE1qNeDp2fHYywt46CHg5z/veN7aCuzfD2ze3PFcCMBqBU6fdk2tfZ1K9e33AgCmTwfuvLPjscMBlJYCf/lLx/PO8Hjs2K2vsz/Tav0QEhyHMYn3Y4D/aAR6DwEAtDta0dxWj0rrUdReOguHox3B/kMwbdJjCNYNhY8muNdrUSiU8NEGY0Ls/8GAgGQcOrUBpef248qV+l7flrvocUBxOBxYvXo1Jk6ciBEjRkiv33///YiKikJYWBiOHTuGtWvXoqioCB988AEAwGKxOIUTANJzi8VyzW1lZmbimWee6WmpRP1a5x95Gg0weTIwaVLHc4cDKC8Htm7teIMUAqivBz76yGWl9nmd3wuVChg8GHj55Y7nQnQcTdmwoeP7AgCXL3d8L67624x6iUKhRFjoSESFj0X8wNkI9h0GjcoPQgjY2xtw6fJZFJbtwPmKwyj5ei8iQsbCPGYZ/P2iEeA1+KbWplZ6YmDgJOiTw1EQ9DFOnvkEVVVF1//CPqjHASU9PR0nTpzA3r17nV5fsWKF9HjkyJEIDQ3FtGnTUFJSgpiYmB5tKyMjA2vWrJGe22w2RERE9Kxwon7u6jfJgQOB9PSO50J0fPwzcWLHc4ej4wjLn//ccfSFel/n90KhAAwG4Be/+HaZ3Q5MmAC0tHR8bxobgX/+E6jgbOk3RO8XhmFD70R06CQMCBgNL3UgVEoPOEQ76q6U4HxtHk6d2YHzFfloulwLAAgJHgK93wAE3uRw0kml1CDIJw5jY4Nh9B+Ko8Xv4syZvWhra74l25eLHgWUVatWYevWrdizZw/Cw8N/tG9KSgoAoLi4GDExMTCZTDh48KBTn6qqKgCAyWS65jq0Wi20Wt62muhmuPpN0s8PmDr122VtbUBcHLBsmWtq62+uPqXB0xO47bZvn7e3A+PGAYsX8zyintBovDFy2FyEhyUhOmgyfLWhUCu1EEKguc2Kcus+lJzbi2MnP8BMYwDuiTbhzwWXYAoZhfghs6D3jIRKoe3V806ux8sjACGG4RgQMhqnT++5ZduVi24FFCEEHn74YWzZsgXZ2dmIjo6+7tfk5+cDAEJDQwEAZrMZv//971FdXQ2j0QgAyMrKgk6nQ3x8fDfLJ1dQqVRSaOxs9fX10jlJ5F6uPk2+rQ2oru547HAAFy8Cjzzimrr6o6u/F+3tHePf3v7t+UK/+x3DSU9FmcZj+OA50HtFQO8ZCUCB1vYruNhUiK9rvsT+r96E1VqBScZgmAP9AQC/HTcV5wc/AYE2+HgYb2k4AQCHaEd14wlUXijol/fv6VZASU9Px8aNG/Hvf/8bfn5+0jkjer0eXl5eKCkpwcaNG3HXXXchMDAQx44dw6OPPorJkycjISEBADB9+nTEx8dj6dKleO6552CxWPDkk08iPT2dR0lkymAwwN/fH/7+/ggICMCAAQMwZMgQxMXFIS4uDkOGDME777yDjRs3Ij8/H1VVVXDDOyj0G53fGiGAS5eAEye+PQfFYnG+0oRurqv/N7lyBcjL+/Z7YbUCL7zAQNJbLlTno6ryDBCmgJdHIJpb6/F19RcoLM1C6df70dp6BQBQZ7fD1toKnUYDqzYYzW31MPkm3vJwAgA2+wVYGypx8szWW75tOehWQFm/fj2AjkuJr/b666/jZz/7GTQaDT777DO8+OKLaGpqQkREBBYsWIAnn3xS6qtSqbB161asXLkSZrMZPj4+SEtLc5o3pauWL1+Ouro6VFVVwWKxoKqqCk38v/mG+Pr6IiIiwqmFh4djwIAB0r8Gw/ev/V+6dCnmzJmDrKwsbNu2DZ988gkuXrzoor2gq3W+Cba3A4WFwJ5vjhQ7HB3nM3z6qetq62+uDodVVcCHH34bSBoagP/7f51DC/Wey811OFL4HlIDf4Xy9lycOL0VFyqOou7SOad+J61WKMrLEebjh5Dxq6AWrjkBq93RgktXinGi4Prh5IG4OCgBvFHUt06mdeubBZaVlUGhUKChoQENDQ2wWq2orq7G2bNnUVpaitLSUpw9exaVlZVo5Vl+36NSqRATE4OhQ4di2LBhiIuLQ1RUFAwGg1PrnBG4q6qrq3Hq1Cm8/fbbeP/991FfX3/zdoKcrFsHzJ3b8dhuB3bsAHbt6nguRMfHN2fPuq6+/mTePOCppzoet7UBR48Cb731bQBpaOg4ekW9L06vx6KYGGwoLkbxd6alCA9Jgt3RgEuXytDW9sMfmygUSkSHT8DYhKXQ+ZkQ5B0HrfrH5+3oTdbmMhSUfYRdX/wJbT/y8c7P4+Px0PDhUAD4/0+dwt/d5Ieqz98sUK/XO+2gEAIOhwOtra1obW1FS0sLWltb0djYiLNnz6KwsBCFhYU4deoUioqKUFtbK32NEEJqfYVCoYBSqYRCoYBKpUJwcDCSkpKklpCQAB8fH2i1Wmg0Gmg0GqhUqhs+lGk0GhEcHIzRo0fj5z//OV566SV89NFHaGho6FPjK0fh4X/C44+/hoKCU9JVOZcvu7qq/snffyE2bfLA22+/Lc1zwimcbr4grRZPJSVBp9EgVqfDqn37UN/y7Rv8+aojXVqPEA6Uln+JhoYqpCQuw+WgizD6jIBOOwAKRbfuEtNtDkcbrM3lOHLy/R8NJwAQazDAU6UC0BHM+hK3Dijf1flGrFKp4HnVDElCCMTExODOb2ZJ6nyTrKmpwZkzZ1BYWIjTp0/j9OnTKC0tRWNjI5qbm9Hc3Ay73Y7m5ma0t8t7Rj+1Wg0vLy94eXnB29sber0ecXFxSExMREJCAkaNGoWwsDAAcAogN+tzVYVCAT8/PyQnJ+Ott97C4cOH8cILL+Dzzz/HxYsX0dLS/074uhXU6gBcuqSRTnQl11EqvdHUxO/FreYAYP9mMhl7ezt8fHzgFxICrVaL2tpaWK1WODonm7kOAYGa+jPYtucpjI7//9AUWQeT/wgEesVCrfS6Kb8/hRBoaq1B6YV9aGi8/g/PL/ftg87DAyqFAo98+WWv1+NKfSqg/JDv/hB1Pg8JCUFISAhuu+pavpaWFlRWVuLcuXMoKyuTWlVVFerr67/XXEGlUsFgMCAwMBBBQUEIDg5GWFgYYmNjpRYdHd2l2wbcbJ1j3RlU9u7di02bNiEnJwfFxcX86I2IelWd3Y4/FxRg9e2343hAANJnzcLMmTMRFhaGjz/+GLt27cKxY8dQXFzc5T+UHKIdX518E5aLyRgedxcuB9chyDcOvhojlIrefRsVaEf95XM4c+7zLs8iuyInp1dr6OTv6YkALy+cs1rR1sVQ15v6RUDpDo1Gg6ioKERFRTm93tTUhJqaGtTU1ODixYuoqalBdXU1LBYLKioqUFFRgcrKSlRWVqKhoaFXa9Lr9VJNAwcORGRkJMLCwhAaGir9e73P8uRAqVRi8uTJSE5OxuHDh7Fz5058+OGHOHny5PW/mIjoOkJDQzFhwgSYzWbEJCXhJ0lJTif1/+d//icWLlyIo0ePYv/+/dizZw/27t2LS5cudWn956sOoba+BENjZmBw9GQEG4YgyDuuV0NKS3sTSiqyUVd77vqdbyKDpycmRUUhxNcXBTU1+LKs7JbXwIDSRT4+PvDx8cHAgQOl14QQaGpqQlNTExobG6V/q6urUVJS4tTKy8u7lNY9PT0xaNAgxMfHY8SIEYiPj0d4eDh0Op3UfH19ofrmM0d35OPjg0mTJmH06NG47777sG3bNrz22msoLi52dWlE5Ga0Wi3Gjh2L+fPnY8KECYiMjERwcDA8PDyu2d/Pzw+33XYbxo8fj4ULF+Ls2bPYsWMH3nnnHZSWll53e1fs9The9G9UVh/H2ISlqPc/h3BdCrw9Am94X4Rw4GLjGZRbDkPtocGouHugVn87/UZp6QHU1n59w9vpCj+NBiG+vgCAuMBA7Csrw60+g5AB5QYoFAr4+vrC19dXup9Q54m2bW1taGtrQ3t7O9ra2nD58mWUlpbi1KlTKCwsREFBAQoLC6FWq5GUlITRo0dj9OjRGDp0KHQ6HdRqNTw8PKBWq906jPwYHx8fjBw5EkOHDsXixYuxYcMG/O1vf0NNTQ3a2tpcXR4RyZBSqYSHhwdCQkKwcOFC3HfffRgyZAg8PT3h4eHR5fNC1Gq1NI1CSkoK1qxZg88++wz/+Mc/kJeXB7vd/oPnHra1N8Ny8RR27PktRg29D4ohSgR4x8DfaxAUUPb43BSHaENJ1WcoLz+MlJH/gciBoxHq23ED3bK6/Th37lCP1tsTFxoacLiyEiONRnxUVHTLwwnAgNLrOi/J7bwqppPBYEBYWBgmdt7o5Drr6C86xyoyMhIZGRlYsWIF/va3v2Hz5s04f/4857UhIgCAv78/TCYTkpKSsHDhQqSmpsLLywvAjf3OVCgU8PT0hFarxaJFi7Bo0SLk5+fjX//6Fz799FNUVlaivr7+GifWCrS2XUbeiTdx3nIIyYn3oSmgGka/EdCqdD2qqarhBMrPH0ZLy2WoVZ5QK7Xw8giAgIBaqQFuYUxwCIFDFRU45MKbPzGg3CL9KXT0ROf4BAUFYd26dUhLS8OmTZuwc+dO7N+/n1f9EPVDPj4+iIuLQ3x8PCZPnow77rgDgwffnBv2Xf07OikpCaNGjUJFRQU+/fRTZGdn48iRIzh16tQ1ju4KVF48jj0HahEbPQVxg2ww6odBp42AUtH1o9/NbfUoq8vF+QvHnNZ9VYXftP6DAYVkR6lUIjo6GmvXrsWCBQvwxRdfYNOmTcjOzu7y5YFE5L4GDx6MKVOmYMKECRg5ciSGDx8uHS25VRQKBQYMGIBly5bh3nvvRUFBAQ4fPoxPP/0Ue/fuRV1dnVN/W2MFjp3agot1xRgxdB4GGBMR6BXbpcndhHDA0nAMBUU70NzcMVmOAopv4onoZ7HkWwwoJFsqlUq618/06dORl5eHP/3pTzh48CCDClEf4+fnhzvuuAM//elPkZSUhNDQUBgMBiiVN3dStK7Q6/Uwm80YN24c5s+fj5KSEnzyySf417/+hYqrPgJpbbuCcxcOot56HjEDJyNh2D0I9h0Gb3XQDx5FF0Kgtuk0DhdswvnzR2/VLrkFBhSSPYVCgcjISAwYMACzZs3Ctm3b8Nvf/hZnzpxBc3MzZ6clckNqtRqenp4YPHgwlixZgvnz5yMsLAwajUaaAVtuVCqVNLVDSkoKnnzySWzduhWvvvoq8vLycOXKFbS1tcHaWIH8k++hrv4c4odNx0DjbQj0GgKV0vnKIiEEmtvqUWLJQWlZLhyOH744QNEPj6MwoJDbUKlU8Pb2xk9+8hPMnz8fb7/9Nt566y0cP34ctbW1PKpCJHNKpRIhISGIiIjAhAkTsGDBAowbN04Wk0p2x9UXQixatAgLFizAsWPHsGHDBnzxxRf4+uuvcfHiRXx9PheX6stgi7dgUMRkhOpGQavWSeemOEQbKuqP4FjRh2houNasseI7270FOycjDCjkllQqFdLS0jB79mxs374dn3zyCT7//HNUc15xItnx9/dHYmIiEhMTMWnSJJjNZunWG32Bh4cHkpOTkZycjLKyMmRnZyM7OxuHDh3C6dOnsffgP1BffwFRkSWICb0DOm04VAoNLl0pxbHi91FZ2ZXJKvtZOgEDCrm5oKAgLF26FHfeeSfy8vKwZcsWfPzxx7h48aKrSyPq90aPHo0777wTEyZMQFxcHAYNGvSDE6j1FZGRkVi6dCkWLFiAkydPSifW7tyZhcrqk6gbXIZh0TNg8I7G19V7UHL2y2t/tPPdwyX9L58woFDfYDKZMGfOHEyYMAFpaWl49dVXsWXLFlzmrXyJbqng4GDMmTMHixYtwpAhQxAUFATfb2Yk7S8UCgV8fHwwbtw4jBo1CvPnz8eFCxfwzjvv4MMtn2B/fgUGx07AngPr0dhU07V1XvXf/oIBhfoMhUKBwMBATJ48GRMmTMCaNWvwxz/+EVlZWWhoaODstES9TKFQQKvVwsfHB6NHj8bSpUsxc+ZMGAwGqFQqaeLK/kyj0SAkJARGoxEJCQlYu3YtsrI+w//+72vw8GiFRqPp4jxP8htHlUoFlUoFtVotzXre+W9oaCgGDx6M6Oho6T5yAwcOhMFgwIABA7q0fgYU6nMUCgU8PDwwevRobNiwAfv378drr70mnbz2Q9NXE1HXaDQahIeHY/Dgwbj99tsxb948DBs2rM/elqM3KBQKqNVqBAQEYOHC+3DffT/FgQMH8P7770u/m2pqamR3VWJnAPX29v5eM5lMGDBgAAYMGICwsDDpsdFohFarveb6bDZbl7fNgEJ9mlqtxm233YakpCQcOHAAW7duxY4dO3Dq1ClXl0bUbSEBAVCr1bjggpPBFQoFwsLCpJNBJ06ciKSkJAQEBNzyWvoChUKB8ePHIyUlBefPn5furLxv3z6cOnUKDtGG1vbLqLnc8buqud0KpfLmvGX7+fkhICAA/v7+8Pf3lx7r9XoEBgYiODgYQUFBCAwMlP4NDAy86ZeDM6BQv+Dj44OpU6di/PjxuO+++/DZZ5/hjTfeQElJiatLI+oSU2AgBoWHQ6VSQaNWo/QW3iNl6tSpmDlzJsaOHYuYmBiEhYXxaEkvUSgUiIiIwJIlSzBv3jycPn0a+fn5OH3YH5dtAdL8J1cabWhoqOrxdgICAqQ5XEwmE8LCwmAymRAaGgqdTifd+NbHx0d67O3tDbXadTGBAYX6FW9vb6SkpCAxMRGLFy/Gpk2b8Nxzz6GxsVF2h1aJrubj5QX1N6FA7+d307bTeV5JQkIC7r77bsyZMwcmkwk6nQ6enp43bbvUcSQjOTkZCQkJaJrfjvpLTXj3nXexYcMGnP26CJevXJL6dh656PxXpVIhODgYUVFRUhs4cCCioqIQHh4OPz8/eHh4XLPJYbbea2FAoX5HoVDAy8sLgwYNwhNPPIFHHnkEzz77LN5++200NTWhqakJzc3Nri6TyEnJ+fPQajTw0mqRX1TUa+tVKpXSX8zx8fGYM2cO7rrrLkRHR0uH8Pv7ia63moeHBwwBHtD7a/HY2hX4efr92LZtG1599VU0NTVJJ5xeHUYiIyPh7e0NwDm8fDfIuBMGFOq3Ov/n9fPzw+9//3v88pe/xP79+7F//34cPnwY58+fR2VlJWpra3kFEMlCwdmzvbIetVoNo9GIiIgIxMTE4Pbbb8eUKVMwaNAglx7SJ2cKhQIqlQp6vR6LFy/G4sWLXV3SLcWfRKJv+Pv7Y9asWZg1axauXLmCoqIiFBYWorCwEAUFBSgoKMCZM2e6eEkgkfxER0cjISEBiYmJSEhIwMiRIxEbG+uWf11T38eAQnQNXl5eGDVqFEaNGoWWlhbU1tbCYrHg/PnzyMvLw5dffokvv/wSdrvd1aUS/ajw8HBMnToV06ZNw9ChQxEaGvqjl4ESyQUDCtF1aDQahIaGIjQ0FImJiUhNTcWVK1dw5coVZGdnY+fOncjOzobFYoHD4eA8K+QySqUSKpUKRqMRc+bMwT333IOkpCR4eXnB29tbmjyNyB0woBB1g1KphJeXF7y8vCCEwP3334/7778fzc3NOHbsGD777DPs3r0bZ8+eRUNDA2w2G1pbW11dNvVh3t7eMBgMMJlMmDJlCubMmQOz2SwdIWEgIXfFgELUQ1f/4vfy8kJKSgpSUlKwdu1alJaWIi8vD3l5eSgqKkJZWRnKy8u7NYsi0Q/R6/XSlRzJycmYPHkykpOT+909b6hvY0Ah6mVqtRqxsbGIjY3F/fffj+rqapw5cwbFxcU4deoUjh8/juPHj6O8vNzVpZIbMRgMGDlyJMaMGYPExEQMGzYMw4YNg99NnBOFyJUYUIhuMqPRCKPRiIkTJ+Ly5cu4ePEiLl68iOLiYuzZswd79uzBqVOneCkzfY9SqcTUqVMxffp0jBs3DuHh4TAajQwl1C8woBDdQt7e3oiMjERERAQSExMxd+5c2O12nD9/HllZWdi5cye++uorNDY2oq2tjSfc9iNKpRJqtRq+vr4YM2YM5s+fj3nz5sHPzw9arRYeHh48n4T6lW7Nb7t+/XokJCRAp9NBp9PBbDZj+/bt0vLm5makp6cjMDAQvr6+WLBgAaqqnO8dUFZWhtmzZ8Pb2xtGoxGPP/44/3KkfqdzAiYvLy8YDAYMHz4cq1evxvbt21FYWIh3330XDz/8MEaPHo3o6GgYDAbZTkdNPdc5YdqwYcPw05/+FK+++iqOHj2KHTt24Oc//zlCQ0Ph5+cHjUbDcEL9TreOoISHh+PZZ59FbGwshBB48803cffdd+PIkSMYPnw4Hn30UXzyySd47733oNfrsWrVKtx777348ssvAQDt7e2YPXs2TCYT9u3bh8rKSjzwwAPw8PDAH/7wh5uyg0Tu4Oo3n6CgIMybNw/z5s1DS0sLjhw5giNHjuDo0aMoLi5GaWkpLly4wOn43ZRGo0FERARiY2MxbNgwjB8/HuPHj0d4eDhDKNFVFOIG75AWEBCA559/Hj/5yU8QHByMjRs34ic/+QkAoLCwEMOGDUNubi7Gjx+P7du3Y86cOaioqEBISAgA4JVXXsHatWtRU1MDjUbTpW3abDbo9XpYrVbodLobKZ/IbbS2tuL8+fM4e/YsSkpKcOLECRw5cgT5+flobGwEAPzzn//ESy+9hKNHj7q4Wlq2bBk0Gg3+8Y9/QKlUIi4uDikpKRgzZgyGDh2KIUOGICIiwtVlEt1S3Xn/7nFAaW9vx3vvvYe0tDQcOXIEFosF06ZNw6VLl2AwGKR+UVFRWL16NR599FGsW7cOH330EfLz86XlpaWlGDRoEA4fPoykpKRrbstutzvN2Gmz2RAREcGAQv1We3s7GhoacOnSJdTW1iIvLw/Z2dkoLS3FiRMncOXKFVeX2O8FBQUhNjYWycnJmDVrFmJiYhAYGAiDwcD73VC/1Z2A0u3/S44fPw6z2Yzm5mb4+vpiy5YtiI+PR35+PjQajVM4AYCQkBBYLBYAgMVikY6cXL28c9kPyczMxDPPPNPdUon6LJVKBYPBAIPBgIEDB2LUqFFYtmwZ2tra4HA4XF0efUOlUkGtVsv6lvZEctXtgBIXF4f8/HxYrVa8//77SEtLQ05Ozs2oTZKRkYE1a9ZIzzuPoBBRx/krarWaf5UTUZ/S7d9oGo0GgwcPBgAkJycjLy8PL730EhYuXIiWlhbU19c7HUWpqqqCyWQCAJhMJhw8eNBpfZ1X+XT2uRatVssbWxEREfUjN3zM0eFwwG63Izk5GR4eHti1a5e0rHOKb7PZDAAwm804fvw4qqurpT5ZWVnQ6XSIj4+/0VKIiIioj+jWEZSMjAzMmjULkZGRaGhowMaNG6W7uer1eixfvhxr1qxBQEAAdDodHn74YZjNZowfPx4AMH36dMTHx2Pp0qV47rnnYLFY8OSTTyI9PZ1HSIiIiEjSrYBSXV2NBx54AJWVldDr9UhISMDOnTtx5513AgBeeOEFKJVKLFiwAHa7HTNmzMDf//536etVKhW2bt2KlStXwmw2w8fHB2lpafjNb37Tu3tFREREbu2G50FxBc6DQkRE5H668/7N696IiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2GFCIiIhIdhhQiIiISHYYUIiIiEh2uhVQ1q9fj4SEBOh0Ouh0OpjNZmzfvl1aPmXKFCgUCqf20EMPOa2jrKwMs2fPhre3N4xGIx5//HG0tbX1zt4QERFRn6DuTufw8HA8++yziI2NhRACb775Ju6++24cOXIEw4cPBwA8+OCD+M1vfiN9jbe3t/S4vb0ds2fPhslkwr59+1BZWYkHHngAHh4e+MMf/tBLu0RERETuTiGEEDeygoCAADz//PNYvnw5pkyZglGjRuHFF1+8Zt/t27djzpw5qKioQEhICADglVdewdq1a1FTUwONRtOlbdpsNuj1elitVuh0uhspn4iIiG6R7rx/9/gclPb2dmzevBlNTU0wm83S6xs2bEBQUBBGjBiBjIwMXL58WVqWm5uLkSNHSuEEAGbMmAGbzYaTJ0/+4LbsdjtsNptTIyIior6rWx/xAMDx48dhNpvR3NwMX19fbNmyBfHx8QCA+++/H1FRUQgLC8OxY8ewdu1aFBUV4YMPPgAAWCwWp3ACQHpusVh+cJuZmZl45plnulsqERERualuB5S4uDjk5+fDarXi/fffR1paGnJychAfH48VK1ZI/UaOHInQ0FBMmzYNJSUliImJ6XGRGRkZWLNmjfTcZrMhIiKix+sjIiIieev2RzwajQaDBw9GcnIyMjMzkZiYiJdeeumafVNSUgAAxcXFAACTyYSqqiqnPp3PTSbTD25Tq9VKVw51NiIiIuq7bngeFIfDAbvdfs1l+fn5AIDQ0FAAgNlsxvHjx1FdXS31ycrKgk6nkz4mIiIiIurWRzwZGRmYNWsWIiMj0dDQgI0bNyI7Oxs7d+5ESUkJNm7ciLvuuguBgYE4duwYHn30UUyePBkJCQkAgOnTpyM+Ph5Lly7Fc889B4vFgieffBLp6enQarU3ZQeJiIjI/XQroFRXV+OBBx5AZWUl9Ho9EhISsHPnTtx5550oLy/HZ599hhdffBFNTU2IiIjAggUL8OSTT0pfr1KpsHXrVqxcuRJmsxk+Pj5IS0tzmjeFiIiI6IbnQXEFzoNCRETkfm7JPChERERENwsDChEREckOAwoRERHJDgMKERERyQ4DChEREckOAwoRERHJDgMKERERyQ4DChEREckOAwoRERHJDgMKERERyQ4DChEREckOAwoRERHJDgMKERERyQ4DChEREckOAwoRERHJDgMKERERyQ4DChEREckOAwoRERHJDgMKERERyQ4DChEREckOAwoRERHJDgMKERERyQ4DChEREckOAwoRERHJDgMKERERyQ4DChEREckOAwoRERHJDgMKERERyQ4DChEREckOAwoRERHJDgMKERERyQ4DChEREckOAwoRERHJjtrVBfSEEAIAYLPZXFwJERERdVXn+3bn+/iPccuA0tDQAACIiIhwcSVERETUXQ0NDdDr9T/aRyG6EmNkxuFwoKioCPHx8SgvL4dOp3N1SW7LZrMhIiKC49gLOJa9h2PZOziOvYdj2TuEEGhoaEBYWBiUyh8/y8Qtj6AolUoMGDAAAKDT6fjD0gs4jr2HY9l7OJa9g+PYeziWN+56R0468SRZIiIikh0GFCIiIpIdtw0oWq0WTz/9NLRaratLcWscx97Dsew9HMvewXHsPRzLW88tT5IlIiKivs1tj6AQERFR38WAQkRERLLDgEJERESyw4BCREREsuOWAeXll1/GwIED4enpiZSUFBw8eNDVJcnOnj17MHfuXISFhUGhUODDDz90Wi6EwLp16xAaGgovLy+kpqbizJkzTn3q6uqwZMkS6HQ6GAwGLF++HI2NjbdwL1wvMzMTY8eOhZ+fH4xGI+655x4UFRU59WlubkZ6ejoCAwPh6+uLBQsWoKqqyqlPWVkZZs+eDW9vbxiNRjz++ONoa2u7lbviUuvXr0dCQoI0yZXZbMb27dul5RzDnnv22WehUCiwevVq6TWOZ9f8+te/hkKhcGpDhw6VlnMcXUy4mc2bNwuNRiP++c9/ipMnT4oHH3xQGAwGUVVV5erSZGXbtm3iv//7v8UHH3wgAIgtW7Y4LX/22WeFXq8XH374oTh69KiYN2+eiI6OFleuXJH6zJw5UyQmJor9+/eLL774QgwePFgsXrz4Fu+Ja82YMUO8/vrr4sSJEyI/P1/cddddIjIyUjQ2Nkp9HnroIRERESF27dolvvrqKzF+/HgxYcIEaXlbW5sYMWKESE1NFUeOHBHbtm0TQUFBIiMjwxW75BIfffSR+OSTT8Tp06dFUVGReOKJJ4SHh4c4ceKEEIJj2FMHDx4UAwcOFAkJCeKRRx6RXud4ds3TTz8thg8fLiorK6VWU1MjLec4upbbBZRx48aJ9PR06Xl7e7sICwsTmZmZLqxK3r4bUBwOhzCZTOL555+XXquvrxdarVZs2rRJCCFEQUGBACDy8vKkPtu3bxcKhUJcuHDhltUuN9XV1QKAyMnJEUJ0jJuHh4d47733pD6nTp0SAERubq4QoiMsKpVKYbFYpD7r168XOp1O2O32W7sDMuLv7y/+93//l2PYQw0NDSI2NlZkZWWJ22+/XQooHM+ue/rpp0ViYuI1l3EcXc+tPuJpaWnBoUOHkJqaKr2mVCqRmpqK3NxcF1bmXkpLS2GxWJzGUa/XIyUlRRrH3NxcGAwGjBkzRuqTmpoKpVKJAwcO3PKa5cJqtQIAAgICAACHDh1Ca2ur01gOHToUkZGRTmM5cuRIhISESH1mzJgBm82GkydP3sLq5aG9vR2bN29GU1MTzGYzx7CH0tPTMXv2bKdxA/gz2V1nzpxBWFgYBg0ahCVLlqCsrAwAx1EO3OpmgRcvXkR7e7vTDwMAhISEoLCw0EVVuR+LxQIA1xzHzmUWiwVGo9FpuVqtRkBAgNSnv3E4HFi9ejUmTpyIESNGAOgYJ41GA4PB4NT3u2N5rbHuXNZfHD9+HGazGc3NzfD19cWWLVsQHx+P/Px8jmE3bd68GYcPH0ZeXt73lvFnsutSUlLwxhtvIC4uDpWVlXjmmWcwadIknDhxguMoA24VUIhcKT09HSdOnMDevXtdXYpbiouLQ35+PqxWK95//32kpaUhJyfH1WW5nfLycjzyyCPIysqCp6enq8txa7NmzZIeJyQkICUlBVFRUXj33Xfh5eXlwsoIcLOreIKCgqBSqb53FnVVVRVMJpOLqnI/nWP1Y+NoMplQXV3ttLytrQ11dXX9cqxXrVqFrVu34vPPP0d4eLj0uslkQktLC+rr6536f3csrzXWncv6C41Gg8GDByM5ORmZmZlITEzESy+9xDHspkOHDqG6uhqjR4+GWq2GWq1GTk4O/vrXv0KtViMkJITj2UMGgwFDhgxBcXExfy5lwK0CikajQXJyMnbt2iW95nA4sGvXLpjNZhdW5l6io6NhMpmcxtFms+HAgQPSOJrNZtTX1+PQoUNSn927d8PhcCAlJeWW1+wqQgisWrUKW7Zswe7duxEdHe20PDk5GR4eHk5jWVRUhLKyMqexPH78uFPgy8rKgk6nQ3x8/K3ZERlyOByw2+0cw26aNm0ajh8/jvz8fKmNGTMGS5YskR5zPHumsbERJSUlCA0N5c+lHLj6LN3u2rx5s9BqteKNN94QBQUFYsWKFcJgMDidRU0dZ/gfOXJEHDlyRAAQf/nLX8SRI0fEuXPnhBAdlxkbDAbx73//Wxw7dkzcfffd17zMOCkpSRw4cEDs3btXxMbG9rvLjFeuXCn0er3Izs52uhTx8uXLUp+HHnpIREZGit27d4uvvvpKmM1mYTabpeWdlyJOnz5d5Ofnix07dojg4OB+dSnir371K5GTkyNKS0vFsWPHxK9+9SuhUCjEp59+KoTgGN6oq6/iEYLj2VWPPfaYyM7OFqWlpeLLL78UqampIigoSFRXVwshOI6u5nYBRQgh/ud//kdERkYKjUYjxo0bJ/bv3+/qkmTn888/FwC+19LS0oQQHZcaP/XUUyIkJERotVoxbdo0UVRU5LSO2tpasXjxYuHr6yt0Op1YtmyZaGhocMHeuM61xhCAeP3116U+V65cEb/4xS+Ev7+/8Pb2FvPnzxeVlZVO6/n666/FrFmzhJeXlwgKChKPPfaYaG1tvcV74zr/8R//IaKiooRGoxHBwcFi2rRpUjgRgmN4o74bUDieXbNw4UIRGhoqNBqNGDBggFi4cKEoLi6WlnMcXUshhBCuOXZDREREdG1udQ4KERER9Q8MKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkOwwoREREJDsMKERERCQ7DChEREQkO/8PiU9BMf1kZi4AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env.reset()\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "\n",
        "    img.set_data(env.render(mode='rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F5paWqo7tWL2"
      },
      "source": [
        "## Policy Gradient\n",
        "Now, we can build a simple policy network. The network will return one of action in the action space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J8tdmeD-tZew"
      },
      "outputs": [],
      "source": [
        "# class PolicyGradientNetwork(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.fc1 = nn.Linear(8, 16)\n",
        "#         self.fc2 = nn.Linear(16, 16)\n",
        "#         self.fc3 = nn.Linear(16, 4)\n",
        "\n",
        "#     def forward(self, state):\n",
        "#         hid = torch.tanh(self.fc1(state))\n",
        "#         hid = torch.tanh(hid)\n",
        "#         return F.softmax(self.fc3(hid), dim=-1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ynbqJrhIFTC3"
      },
      "source": [
        "Then, we need to build a simple agent. The agent will acts according to the output of the policy network above. There are a few things can be done by agent:\n",
        "- `learn()`：update the policy network from log probabilities and rewards.\n",
        "- `sample()`：After receiving observation from the environment, utilize policy network to tell which action to take. The return values of this function includes action and log probabilities. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zZo-IxJx286z"
      },
      "outputs": [],
      "source": [
        "# from torch.optim.lr_scheduler import StepLR\n",
        "# class PolicyGradientAgent():\n",
        "    \n",
        "#     def __init__(self, network):\n",
        "#         self.network = network\n",
        "#         self.optimizer = optim.SGD(self.network.parameters(), lr=0.002)\n",
        "        \n",
        "#     def forward(self, state):\n",
        "#         return self.network(state)\n",
        "#     def learn(self, log_probs, rewards):\n",
        "#         loss = (-log_probs * rewards).sum() # You don't need to revise this to pass simple baseline (but you can)\n",
        "\n",
        "#         self.optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         self.optimizer.step()\n",
        "        \n",
        "#     def sample(self, state):\n",
        "#         action_prob = self.network(torch.FloatTensor(state))\n",
        "#         action_dist = Categorical(action_prob)\n",
        "#         action = action_dist.sample()\n",
        "#         log_prob = action_dist.log_prob(action)\n",
        "#         return action.item(), log_prob"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ehPlnTKyRZf9"
      },
      "source": [
        "Lastly, build a network and agent to start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GfJIvML-RYjL"
      },
      "outputs": [],
      "source": [
        "# network = PolicyGradientNetwork()\n",
        "# agent = PolicyGradientAgent(network)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actor-Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.optim.lr_scheduler import StepLR\n",
        "# class ActorCritic(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(8, 16),\n",
        "#             nn.Tanh(),\n",
        "#             nn.Linear(16, 16),\n",
        "#             nn.Tanh()\n",
        "#         )\n",
        "        \n",
        "#         self.actor = nn.Linear(16, 4)\n",
        "#         self.critic = nn.Linear(16, 1)\n",
        "        \n",
        "#         self.values = []\n",
        "#         self.optimizer = optim.SGD(self.parameters(), lr=0.001)\n",
        "#         self.scheduler = optim.lr_scheduler.CyclicLR(self.optimizer, \n",
        "#                                                      base_lr=2e-4, max_lr=2e-3, \n",
        "#                                                      step_size_up=10, mode='triangular2')\n",
        "        \n",
        "        \n",
        "#     def forward(self, state):\n",
        "#         hid = self.fc(state)\n",
        "#         self.values.append(self.critic(hid).squeeze(-1))\n",
        "#         return F.softmax(self.actor(hid), dim=-1)\n",
        "    \n",
        "#     def learn(self, log_probs, rewards):\n",
        "#         values = torch.stack(self.values)\n",
        "#         loss = (-log_probs * (rewards - values.detach())).sum() + F.smooth_l1_loss(values, rewards)\n",
        "#         self.optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         self.optimizer.step()\n",
        "#         self.scheduler.step()\n",
        "        \n",
        "#         self.values = []\n",
        "        \n",
        "#     def sample(self, state):\n",
        "#         action_prob = self(torch.FloatTensor(state))\n",
        "#         action_dist = Categorical(action_prob)\n",
        "#         action = action_dist.sample()\n",
        "#         log_prob = action_dist.log_prob(action)\n",
        "#         return action.item(), log_prob\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# agent = ActorCritic()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DQN\n",
        "- reference: https://zhuanlan.zhihu.com/p/514067644 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(8, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 4)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "    \n",
        "class ReplayMemory: #Fixed-size buffer to store experience tuples.\n",
        "    def __init__(self, CAPACITY):\n",
        "        self.capacity = CAPACITY  \n",
        "        self.memory = []  \n",
        "        self.index = 0  \n",
        "        self.transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "        \n",
        "    def push(self, state, action, state_next, reward): # Push a new experience to memory\n",
        "        if len(self.memory) < self.capacity: # if still has capacity, initialize memory[index] to None\n",
        "            self.memory.append(None)\n",
        "\n",
        "        self.memory[self.index] = self.transition(state, action, state_next, reward)\n",
        "\n",
        "        self.index = (self.index + 1) % self.capacity  # circular index\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DQN Agent\n",
        "- https://blog.csdn.net/Raphael9900/article/details/128549103 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.optim.lr_scheduler import StepLR\n",
        "class DQNAgent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        \"\"\"Initialize an Agent object.\"\"\"\n",
        "        \n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        # Replay memory\n",
        "        self.memory_capacity = 10000\n",
        "        self.memory = ReplayMemory(self.memory_capacity)\n",
        "        \n",
        "        # Q-Network\n",
        "        self.main_q_network = DQN() \n",
        "        self.target_q_network = DQN()\n",
        "        \n",
        "        # optimizer\n",
        "        self.optimizer = optim.RMSprop(self.main_q_network.parameters(), lr=1e-4)\n",
        "        # self.scheduler = optim.lr_scheduler.CyclicLR(self.optimizer, base_lr=2e-4, max_lr=2e-3, step_size_up=10, mode='triangular2')\n",
        "    \n",
        "    def update_q_function(self):\n",
        "        '''update q function'''\n",
        "        \n",
        "        # no enough samples, just return\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        # If enough samples are available in memory, get random subset and learn\n",
        "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.non_final_next_states = self.make_minibatch()\n",
        "        \n",
        "        self.expected_state_action_values = self.get_expected_state_action_values()\n",
        "\n",
        "        self.update_main_q_network()\n",
        "\n",
        "    def make_minibatch(self):\n",
        "        '''Creating a mini-batch'''\n",
        "\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "\n",
        "        Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                           if s is not None])\n",
        "\n",
        "        return batch, state_batch, action_batch, reward_batch, non_final_next_states\n",
        "\n",
        "    def get_expected_state_action_values(self):\n",
        "        '''calculate Q（St,at）'''\n",
        "\n",
        "        self.main_q_network.eval()\n",
        "        self.target_q_network.eval()\n",
        "\n",
        "        self.state_action_values = self.main_q_network(\n",
        "            self.state_batch).gather(1, self.action_batch)\n",
        "\n",
        "        non_final_mask = torch.BoolTensor(tuple(map(lambda s: s is not None,\n",
        "                                                    self.batch.next_state)))\n",
        "        # set all state to 0\n",
        "        next_state_values = torch.zeros(BATCH_SIZE)\n",
        "\n",
        "        next_state_values[non_final_mask] = self.target_q_network(\n",
        "            self.non_final_next_states).max(1)[0].detach()\n",
        "        # DQN formula\n",
        "        expected_state_action_values = self.reward_batch + GAMMA * next_state_values\n",
        "        \n",
        "        return expected_state_action_values \n",
        "        \n",
        "    def get_action(self, state, episode, test=False):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        if test:\n",
        "            self.main_q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                # t.max(1) will return largest column value of each row.\n",
        "                # second column on max result is index of where max element was\n",
        "                # found, so we pick action with the larger expected reward.\n",
        "                action = self.main_q_network(torch.from_numpy(state).unsqueeze(0)).max(1)[1].view(1, 1)\n",
        "            return action.item()\n",
        "        \n",
        "        global steps_done\n",
        "        # Epsilon-greedy policy\n",
        "        #epsilon = episode\n",
        "        #epsilon = 0.5 * (1 / (episode + 1))\n",
        "        epsilon = EPS_END + (EPS_START - EPS_END) * \\\n",
        "                np.exp(-1. * steps_done / EPS_DECAY)\n",
        "        #print('epsilon', epsilon)\n",
        "        \n",
        "        steps_done += 1\n",
        "        \n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            #print('use max')\n",
        "            self.main_q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                # t.max(1) will return largest column value of each row.\n",
        "                # second column on max result is index of where max element was\n",
        "                # found, so we pick action with the larger expected reward.\n",
        "                action = self.main_q_network(state).max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            #print('random')\n",
        "            action = torch.LongTensor(\n",
        "                [[random.randrange(self.num_actions)]])  \n",
        "            \n",
        "        return action\n",
        "\n",
        "    def update_main_q_network(self):\n",
        "        \n",
        "        '''update main q net'''\n",
        "\n",
        "        # set train mode\n",
        "        self.main_q_network.train()\n",
        "        # Hurberloss function\n",
        "        # expected_state_action_values (minbatch,)->(minbatchx1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(self.state_action_values,\n",
        "                                self.expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        # update\n",
        "        self.optimizer.zero_grad()  # reset gradient\n",
        "        loss.backward()  # backpropagation\n",
        "        for param in self.main_q_network.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "            \n",
        "        self.optimizer.step()\n",
        "        # self.scheduler.step()\n",
        "\n",
        "\n",
        "    def memorize(self, state, action, state_next, reward):\n",
        "        '''save state, action, state_next, reward into replay memory'''\n",
        "        self.memory.push(state, action, state_next, reward)\n",
        "\n",
        "    def update_target_q_function(self):\n",
        "        \n",
        "        '''synchronize Target Q-Network to Main Q-Network'''\n",
        "        self.target_q_network.load_state_dict(self.main_q_network.state_dict())  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "network = DQN()\n",
        "agent = DQNAgent(env.observation_space.shape[0], env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# class Action(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super().__init__()\n",
        "#     self.fc1 = nn.Linear(8, 64)\n",
        "#     self.fc2 = nn.Linear(64, 64)\n",
        "#     self.fc3 = nn.Linear(64, 4)\n",
        "\n",
        "#   def forward(self, state):\n",
        "#       hid = torch.tanh(self.fc1(state))\n",
        "#       hid = torch.tanh(self.fc2(hid))\n",
        "#       return F.softmax(self.fc3(hid), dim=-1)\n",
        "  \n",
        "  \n",
        "# class Critic(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super().__init__()\n",
        "#     self.fc1 = nn.Linear(8, 64)\n",
        "#     self.fc2 = nn.Linear(64, 64)\n",
        "#     self.fc3 = nn.Linear(64, 4)\n",
        "\n",
        "#   def forward(self, state):\n",
        "#       hid = torch.tanh(self.fc1(state))\n",
        "#       hid = torch.tanh(self.fc2(hid))\n",
        "#       return F.softmax(self.fc3(hid), dim=-1)\n",
        "  \n",
        "\n",
        "# class DQN():\n",
        "#     def __init__(self,action,critic):\n",
        "#         self.action_net = action\n",
        "#         self.critic_net = critic\n",
        "        \n",
        "#         self.optimizer = optim.AdamW(self.action_net.parameters(), lr=5e-4) #originally is Adam\n",
        "#         self.scheduler = optim.lr_scheduler.CyclicLR(self.optimizer, base_lr=2e-4, max_lr=2e-3, step_size_up=10, mode='triangular2')\n",
        "        \n",
        "#         self.critic_net.load_state_dict(self.action_net.state_dict()) \n",
        "#         self.critic_net.eval() \n",
        "        \n",
        "#     def forward(self, state):\n",
        "#         return self.action_net(state)\n",
        "    \n",
        "#     def learn(self, state_action_values, expected_state_action_values,batch):\n",
        "#         loss = torch.zeros(1)\n",
        "        \n",
        "#         for i in range(len(state_action_values)):\n",
        "#             loss += F.smooth_l1_loss(state_action_values[i], expected_state_action_values[i])\n",
        "#         self.optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "        \n",
        "#         self.optimizer.step()\n",
        "#         self.scheduler.step()\n",
        "    \n",
        "#         if batch%10 == 9:\n",
        "#             self.critic_net.load_state_dict(self.action_net.state_dict())\n",
        "            \n",
        "#     def sample(self, state):\n",
        "#         r = torch.randn(1)\n",
        "#         if r > 0.2:\n",
        "#             with torch.no_grad():\n",
        "#                 state = torch.FloatTensor(state).to(device)\n",
        "#                 return self.action_net(state).argmax().item()\n",
        "#         else:\n",
        "#             return torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long).item()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ouv23glgf5Qt"
      },
      "source": [
        "## Training Agent\n",
        "\n",
        "Now let's start to train our agent.\n",
        "Through taking all the interactions between agent and environment as training data, the policy network can learn from all these attempts.  \n",
        "Reference is the same.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> for DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53e768da16134d3bafb6de3fb2954686",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/600 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.from_numpy(rewards) looks like  torch.Size([590])\n",
            "-433.39037110942786\n",
            "length of actions is  78\n",
            "-251.44489637172776\n",
            "length of actions is  105\n",
            "-160.14698323218713\n",
            "length of actions is  80\n",
            "-138.42770454733193\n",
            "length of actions is  80\n",
            "-75.65927283442142\n",
            "length of actions is  160\n",
            "Your final reward is : -211.81\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1494])\n",
            "-76.05466074140702\n",
            "length of actions is  114\n",
            "-130.52175748193156\n",
            "length of actions is  1000\n",
            "-171.40800419825675\n",
            "length of actions is  215\n",
            "-212.41146666366666\n",
            "length of actions is  160\n",
            "-97.07558335921664\n",
            "length of actions is  1000\n",
            "Your final reward is : -137.49\n",
            "torch.from_numpy(rewards) looks like  torch.Size([1696])\n",
            "-265.7831182452813\n",
            "length of actions is  85\n",
            "-485.3474027006262\n",
            "length of actions is  210\n",
            "-173.64272164835734\n",
            "length of actions is  1000\n",
            "-104.73683502114832\n",
            "length of actions is  198\n",
            "-368.96787844821563\n",
            "length of actions is  226\n",
            "Your final reward is : -279.70\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(state, batch)  \n\u001b[0;32m---> 38\u001b[0m     observation_next, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     40\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     41\u001b[0m     total_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/sheng-ru/lib/python3.10/site-packages/gym/wrappers/time_limit.py:60\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     49\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m        \"TimeLimit.truncated\"=False if the environment terminated\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action),\n\u001b[1;32m     61\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
            "File \u001b[0;32m~/anaconda3/envs/sheng-ru/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
            "File \u001b[0;32m~/anaconda3/envs/sheng-ru/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:52\u001b[0m, in \u001b[0;36mStepAPICompatibility.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     44\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment, returning 5 or 4 items depending on `new_step_api`.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_step_api:\n\u001b[1;32m     54\u001b[0m         \u001b[39mreturn\u001b[39;00m step_to_new_api(step_returns)\n",
            "File \u001b[0;32m~/anaconda3/envs/sheng-ru/lib/python3.10/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
            "File \u001b[0;32m~/anaconda3/envs/sheng-ru/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:555\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    544\u001b[0m     p\u001b[39m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    545\u001b[0m         (ox \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power, oy \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power),\n\u001b[1;32m    546\u001b[0m         impulse_pos,\n\u001b[1;32m    547\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[1;32m    549\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    550\u001b[0m         (\u001b[39m-\u001b[39mox \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power, \u001b[39m-\u001b[39moy \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power),\n\u001b[1;32m    551\u001b[0m         impulse_pos,\n\u001b[1;32m    552\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    553\u001b[0m     )\n\u001b[0;32m--> 555\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworld\u001b[39m.\u001b[39;49mStep(\u001b[39m1.0\u001b[39;49m \u001b[39m/\u001b[39;49m FPS, \u001b[39m6\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m, \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m)\n\u001b[1;32m    557\u001b[0m pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mposition\n\u001b[1;32m    558\u001b[0m vel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mlinearVelocity\n",
            "File \u001b[0;32m~/anaconda3/envs/sheng-ru/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:60\u001b[0m, in \u001b[0;36mContactDetector.BeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     57\u001b[0m     contactListener\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env\n\u001b[0;32m---> 60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mBeginContact\u001b[39m(\u001b[39mself\u001b[39m, contact):\n\u001b[1;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlander \u001b[39m==\u001b[39m contact\u001b[39m.\u001b[39mfixtureA\u001b[39m.\u001b[39mbody\n\u001b[1;32m     63\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlander \u001b[39m==\u001b[39m contact\u001b[39m.\u001b[39mfixtureB\u001b[39m.\u001b[39mbody\n\u001b[1;32m     64\u001b[0m     ):\n\u001b[1;32m     65\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mgame_over \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPISODE_PER_BATCH = 5\n",
        "NUM_BATCH = 600        \n",
        "BATCH_SIZE = 32\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "GAMMA = 0.99\n",
        "\n",
        "best_score = 0\n",
        "best_batch = 0\n",
        "\n",
        "# set the training mode\n",
        "agent.main_q_network.train()\n",
        "agent.target_q_network.train()\n",
        "steps_done = 0\n",
        "\n",
        "avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "for batch in prg_bar:\n",
        "\n",
        "    rewards = []\n",
        "    total_rewards, final_rewards = [], []\n",
        "\n",
        "    # 蒐集訓練資料\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "        observation = env.reset() \n",
        "        state = observation  \n",
        "        state = torch.from_numpy(state).type(\n",
        "                torch.FloatTensor)\n",
        "        state = torch.unsqueeze(state, 0)\n",
        "        total_reward, total_step = 0, 0\n",
        "\n",
        "        while True:\n",
        "\n",
        "            action = agent.get_action(state, batch)  \n",
        "            observation_next, reward, done, _ = env.step(action.item())\n",
        "\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "            rewards.append(reward) \n",
        "            # ! 重要 ！\n",
        "            # 現在的reward 的implementation 為每個時刻的瞬時reward, 給定action_list : a1, a2, a3 ......\n",
        "            #                                                       reward :     r1, r2 ,r3 ......\n",
        "            # medium：將reward調整成accumulative decaying reward, 給定action_list : a1,                         a2,                           a3 ......\n",
        "            #                                                       reward :     r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,r3+0.99*r4+0.99^2*r5+ ......\n",
        "            \n",
        "            # boss : implement DQN\n",
        "            if done:\n",
        "                state_next = None  \n",
        "            else:\n",
        "                state_next = observation_next  \n",
        "                state_next = torch.from_numpy(state_next).type(\n",
        "                        torch.FloatTensor)  \n",
        "                state_next = torch.unsqueeze(state_next, 0)\n",
        "                \n",
        "            agent.memorize(state, action, state_next, torch.FloatTensor([reward]))\n",
        "            \n",
        "            agent.update_q_function()\n",
        "            \n",
        "            state = state_next\n",
        "        \n",
        "            if done:\n",
        "                final_rewards.append(reward)\n",
        "                total_rewards.append(total_reward)\n",
        "                break\n",
        "                \n",
        "\n",
        "    #print(f\"rewards looks like \", np.shape(rewards))      \n",
        "    # 紀錄訓練過程\n",
        "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "    avg_total_rewards.append(avg_total_reward)\n",
        "    avg_final_rewards.append(avg_final_reward)\n",
        "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "\n",
        "    # 更新網路\n",
        "    agent.update_target_q_function()\n",
        "    print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(np.array(rewards)).size())\n",
        "\n",
        "\n",
        "    ### testing\n",
        "\n",
        "    fix(env, seed)\n",
        "    agent.main_q_network.eval() \n",
        "    NUM_OF_TEST = 5 # Do not revise it !!!!!\n",
        "    test_total_reward = []\n",
        "    action_list = []\n",
        "    for i in range(NUM_OF_TEST):\n",
        "        actions = []\n",
        "        state = env.reset()\n",
        "\n",
        "        #img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "        total_reward = 0\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.get_action(state, episode=i, test=True)\n",
        "            actions.append(action)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            \n",
        "        print(total_reward)\n",
        "        test_total_reward.append(total_reward)\n",
        "\n",
        "        action_list.append(actions)\n",
        "        print(\"length of actions is \", len(actions))\n",
        "        \n",
        "    print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))\n",
        "    if np.mean(test_total_reward) > 250:\n",
        "        distribution = {}\n",
        "        for actions in action_list:\n",
        "            for action in actions:\n",
        "                if action not in distribution.keys():\n",
        "                    distribution[action] = 1\n",
        "                else:\n",
        "                    distribution[action] += 1\n",
        "                    \n",
        "        PATH = \"Action_List_test\" + str(batch) + \".npy\" \n",
        "        np.save(PATH ,np.array(action_list)) \n",
        "        \n",
        "        if np.mean(test_total_reward) > best_score:\n",
        "            best_score = np.mean(test_total_reward)\n",
        "            best_batch = batch\n",
        "            print('Improve to score %.2f at batch %d'% (best_score, best_batch ))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> for actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0140a8d9e1b34c27915aa19e1a1a34f3",
            "594979b124dd49839f867b3cd6d067b9",
            "be9995b36a9c49059e5e4a7d3600768e",
            "b19bcb018139401993ee4997afdc0610",
            "07e1a5ba040f4d268158170b4a51acaa",
            "3e594e39cd5945c29de6ef25ae3d3d53",
            "8c51c6df10bb43d0a36b1bd4fb3d78db",
            "fd372c915a0a438cb0d624af6fae5906",
            "dbcc8f82bf2d415782d810393e6d11f7",
            "0c4bd638d94548b5b1907f06ea5e5af8",
            "fb8055227cfc4184b00f14bd531e70d5"
          ]
        },
        "id": "vg5rxBBaf38_",
        "outputId": "b2b17b04-b890-488d-da00-ac5428eb98a0"
      },
      "outputs": [],
      "source": [
        "# agent.train()  # Switch network into training mode \n",
        "# EPISODE_PER_BATCH = 5  # update the  agent every 5 episode\n",
        "# NUM_BATCH = 600       # totally update the agent for 400 time\n",
        "# rate = 0.985\n",
        "# avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "# prg_bar = tqdm(range(NUM_BATCH))\n",
        "# for batch in prg_bar:\n",
        "\n",
        "#     log_probs, rewards = [], []\n",
        "#     total_rewards, final_rewards = [], []\n",
        "\n",
        "#     # collect trajectory\n",
        "#     for episode in range(EPISODE_PER_BATCH):\n",
        "        \n",
        "#         state = env.reset()\n",
        "#         total_reward, total_step = 0, 0\n",
        "#         seq_rewards = []\n",
        "#         while True:\n",
        "\n",
        "#             action, log_prob = agent.sample(state) # at, log(at|st)\n",
        "#             next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "#             log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
        "#             seq_rewards.append(reward)\n",
        "#             state = next_state\n",
        "#             total_reward += reward\n",
        "#             total_step += 1\n",
        "#             #rewards.append(reward) # change here\n",
        "#             # ! IMPORTANT !\n",
        "#             # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
        "#             #                                                         rewards :     r1, r2 ,r3 ......\n",
        "#             # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
        "#             #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
        "#             # boss : implement Actor-Critic\n",
        "#             if done:\n",
        "#                 final_rewards.append(reward)\n",
        "#                 total_rewards.append(total_reward)\n",
        "#                 # calculate accumulative rewards\n",
        "#                 for i in range(2, len(seq_rewards)+1):\n",
        "#                     seq_rewards[-i] += rate * (seq_rewards[-i+1])\n",
        "#                 rewards += seq_rewards\n",
        "                \n",
        "#                 break\n",
        "\n",
        " \n",
        "#     print(f\"rewards looks like \", np.shape(rewards))  \n",
        "#     print(f\"log_probs looks like \", len(log_probs))     \n",
        "#     # record training process\n",
        "#     avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "#     avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "#     avg_total_rewards.append(avg_total_reward)\n",
        "#     avg_final_rewards.append(avg_final_reward)\n",
        "#     prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "\n",
        "#     # update agent\n",
        "#     # rewards = np.concatenate(rewards, axis=0)\n",
        "#     rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward \n",
        "#     agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
        "#     print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
        "#     print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vNb_tuFYhKVK"
      },
      "source": [
        "### Training Result\n",
        "During the training process, we recorded `avg_total_reward`, which represents the average total reward of episodes before updating the policy network.\n",
        "\n",
        "Theoretically, if the agent becomes better, the `avg_total_reward` will increase.\n",
        "The visualization of the training process is shown below:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "wZYOI8H10SHN",
        "outputId": "5ee3f439-e119-42d9-c18d-180542db2d82"
      },
      "outputs": [],
      "source": [
        "plt.plot(avg_total_rewards)\n",
        "plt.title(\"Total Rewards\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mV5jj4dThz0Y"
      },
      "source": [
        "In addition, `avg_final_reward` represents average final rewards of episodes. To be specific, final rewards is the last reward received in one episode, indicating whether the craft lands successfully or not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "txDZ5vlGWz5w",
        "outputId": "fe57f153-e301-4efd-b01e-ff48f6d5c167"
      },
      "outputs": [],
      "source": [
        "plt.plot(avg_final_rewards)\n",
        "plt.title(\"Final Rewards\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u2HaGRVEYGQS"
      },
      "source": [
        "## Testing\n",
        "The testing result will be the average reward of 5 testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "5yFuUKKRYH73",
        "outputId": "d145051c-990c-43ed-b2a1-0136ed205d0e"
      },
      "outputs": [],
      "source": [
        "fix(env, seed)\n",
        "# agent.network.eval()  # set the network into evaluation mode\n",
        "agent.eval()\n",
        "NUM_OF_TEST = 5 # Do not revise this !!!\n",
        "test_total_reward = []\n",
        "action_list = []\n",
        "for i in range(NUM_OF_TEST):\n",
        "  actions = []\n",
        "  state = env.reset()\n",
        "\n",
        "  img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "      action, _ = agent.sample(state)\n",
        "      actions.append(action)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "\n",
        "      total_reward += reward\n",
        "\n",
        "      img.set_data(env.render(mode='rgb_array'))\n",
        "      display.display(plt.gcf())\n",
        "      display.clear_output(wait=True)\n",
        "      \n",
        "  print(total_reward)\n",
        "  test_total_reward.append(total_reward)\n",
        "\n",
        "  action_list.append(actions) # save the result of testing \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aex7mcKr0J01",
        "outputId": "64e05837-0944-4232-c324-5913da3154d8"
      },
      "outputs": [],
      "source": [
        "print(np.mean(test_total_reward))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "leyebGYRpqsF"
      },
      "source": [
        "Action list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGAH4YWDpp4u",
        "outputId": "afc1758c-303c-455a-ad40-957256b6262c"
      },
      "outputs": [],
      "source": [
        "print(\"Action list looks like \", action_list)\n",
        "print(\"Action list's shape looks like \", np.shape(action_list))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fNkmwucrHMen"
      },
      "source": [
        "Analysis of actions taken by agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHdAItjj1nxw",
        "outputId": "20b31444-fbdb-43e3-ee44-5a3dadea290e"
      },
      "outputs": [],
      "source": [
        "distribution = {}\n",
        "for actions in action_list:\n",
        "  for action in actions:\n",
        "    if action not in distribution.keys():\n",
        "      distribution[action] = 1\n",
        "    else:\n",
        "      distribution[action] += 1\n",
        "print(distribution)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ricE0schY75M"
      },
      "source": [
        "## Saving the result of Model Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZsMkGmIY42b",
        "outputId": "98e58ed5-e6b4-4f83-8d50-eba66ddd4dd3"
      },
      "outputs": [],
      "source": [
        "PATH = \"/home/wmnlab/Documents/sheng-ru/bai/test3/Action_List.npy\" # Can be modified into the name or path you want\n",
        "np.save(PATH ,np.array(action_list)) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "seT4NUmWmAZ1"
      },
      "source": [
        "# Server \n",
        "The code below simulate the environment on the judge server. Can be used for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "U69c-YTxaw6b",
        "outputId": "4856e8a2-b2e7-4603-aa8b-fe309091f4d6"
      },
      "outputs": [],
      "source": [
        "action_list = np.load(PATH,allow_pickle=True) # The action list you upload\n",
        "seed = 2023 # Do not revise this\n",
        "fix(env, seed)\n",
        "\n",
        "# agent.network.eval()  # set network to evaluation mode\n",
        "agent.eval()\n",
        "\n",
        "test_total_reward = []\n",
        "if len(action_list) != 5:\n",
        "  print(\"Wrong format of file !!!\")\n",
        "  exit(0)\n",
        "for actions in action_list:\n",
        "  state = env.reset()\n",
        "  img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "\n",
        "  for action in actions:\n",
        "  \n",
        "      state, reward, done, _ = env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "  print(f\"Your reward is : %.2f\"%total_reward)\n",
        "  test_total_reward.append(total_reward)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TjFBWwQP1hVe"
      },
      "source": [
        "# Your score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpJpZz3Wbm0X",
        "outputId": "497f775f-506a-441f-bf42-b7cff43ed627"
      },
      "outputs": [],
      "source": [
        "print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wUBtYXG2eaqf"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Below are some useful tips for you to get high score.\n",
        "\n",
        "- [DRL Lecture 1: Policy Gradient (Review)](https://youtu.be/z95ZYgPgXOY)\n",
        "- [ML Lecture 23-3: Reinforcement Learning (including Q-learning) start at 30:00](https://youtu.be/2-JNBzCq77c?t=1800)\n",
        "- [Lecture 7: Policy Gradient, David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sheng-ru",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "c7771dd1fbefba0f9e49b3f12d6cb05ea3fc9d8cb4bbb591d0ecb9d07210ade7"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0140a8d9e1b34c27915aa19e1a1a34f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_594979b124dd49839f867b3cd6d067b9",
              "IPY_MODEL_be9995b36a9c49059e5e4a7d3600768e",
              "IPY_MODEL_b19bcb018139401993ee4997afdc0610"
            ],
            "layout": "IPY_MODEL_07e1a5ba040f4d268158170b4a51acaa"
          }
        },
        "07e1a5ba040f4d268158170b4a51acaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c4bd638d94548b5b1907f06ea5e5af8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e594e39cd5945c29de6ef25ae3d3d53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "594979b124dd49839f867b3cd6d067b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e594e39cd5945c29de6ef25ae3d3d53",
            "placeholder": "​",
            "style": "IPY_MODEL_8c51c6df10bb43d0a36b1bd4fb3d78db",
            "value": "Total: -179.2, Final: -100.0: 100%"
          }
        },
        "8c51c6df10bb43d0a36b1bd4fb3d78db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b19bcb018139401993ee4997afdc0610": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c4bd638d94548b5b1907f06ea5e5af8",
            "placeholder": "​",
            "style": "IPY_MODEL_fb8055227cfc4184b00f14bd531e70d5",
            "value": " 100/100 [01:11&lt;00:00,  1.65s/it]"
          }
        },
        "be9995b36a9c49059e5e4a7d3600768e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd372c915a0a438cb0d624af6fae5906",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dbcc8f82bf2d415782d810393e6d11f7",
            "value": 100
          }
        },
        "dbcc8f82bf2d415782d810393e6d11f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb8055227cfc4184b00f14bd531e70d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd372c915a0a438cb0d624af6fae5906": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
