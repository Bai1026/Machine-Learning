{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n### LifeLong Machine Learning\n### TA's Slide\n[Slide](https://docs.google.com/presentation/d/1SMJLWPTPCIrZdNdAjrS4zQZx1kfB73jCFSb7JRX90gQ/edit?usp=sharing)\n\n### Definition\nThe detailed explanations and definitions of LifeLong Learning please refer to [LifeLong learning](https://youtu.be/7qT5P9KJnWo) \n\n\n### Methods\nSomeone proposed a survey paper for LifeLong Learning at the end of 2019 to distinguish 2016-2019 LigeLong Learning methods into three families.\n\nWe can distinguish LifeLong Learning methods into three families, based on how task\nspecific information is stored and used throughout the sequential learning process:\n* Replay-based methods\n* Regularization-based methods\n* Parameter isolation methods\n\n<img src=\"https://i.ibb.co/VDFJkWG/2019-12-29-17-25.png\" width=\"100%\">\n\nIn this assignment, we have to go through EWC, MAS, SI, Remanian Walk, SCP Methods in the prior-focused methods of the regularization-based methods. \n\nSource: [Continual Learning in Neural\nNetworks](https://arxiv.org/pdf/1910.02718.pdf)\n\nPlease feel free to mail us if you have any questions.\n\nmlta-2023-spring@googlegroups.com\n","metadata":{}},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import argparse\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport torch\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nimport torch.utils.data.sampler as sampler\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import datasets, transforms\nimport tqdm\nfrom tqdm import trange","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check devices","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fix Random Seeds","metadata":{}},{"cell_type":"code","source":"def same_seeds(seed):\n  torch.manual_seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  \n  np.random.seed(seed)  \n  torch.backends.cudnn.benchmark = False\n  torch.backends.cudnn.deterministic = True\n\nsame_seeds(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Data\nWe utilize rotated MNIST as our training dataset.\n\nSo, first, we utilize 5 different rotations to generate 10 different rotated MNISTs as different tasks.","metadata":{}},{"cell_type":"markdown","source":"### Rotation and Transformation","metadata":{}},{"cell_type":"code","source":"# Rotate MNIST to generate 10 tasks\n\ndef _rotate_image(image, angle):\n  if angle is None:\n    return image\n\n  image = transforms.functional.rotate(image, angle=angle)\n  return image\n\ndef get_transform(angle=None):\n  transform = transforms.Compose([transforms.ToTensor(),\n                   transforms.Lambda(lambda x: _rotate_image(x, angle)),\n                   Pad(28)\n                   ])\n  return transform\n\nclass Pad(object):\n  def __init__(self, size, fill=0, padding_mode='constant'):\n    self.size = size\n    self.fill = fill\n    self.padding_mode = padding_mode\n    \n  def __call__(self, img):\n    # If the H and W of img is not equal to desired size,\n    # then pad the channel of img to desired size.\n    img_size = img.size()[1]\n    assert ((self.size - img_size) % 2 == 0)\n    padding = (self.size - img_size) // 2\n    padding = (padding, padding, padding, padding)\n    return F.pad(img, padding, self.padding_mode, self.fill)\n\nclass Data():\n  def __init__(self, path, train=True, angle=None):\n    transform = get_transform(angle)\n    self.dataset = datasets.MNIST(root=os.path.join(path, \"MNIST\"), transform=transform, train=train, download=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloaders and Arguments\n- Training Arguments\n- Setup 5 different Rotations\n- 5 Train DataLoader\n- 5 Test DataLoader ","metadata":{}},{"cell_type":"code","source":"class Args:\n  task_number = 5\n  epochs_per_task = 10\n  lr = 1.0e-4\n  batch_size = 128\n  test_size=8192\n\nargs=Args()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# generate rotations for the tasks.\n\n# generate rotated MNIST data from 10 different rotations.\n\nangle_list = [20 * x for x in range(args.task_number)]\n\n# prepare rotated MNIST datasets.\n\ntrain_datasets = [Data('data', angle=angle_list[index]) for index in range(args.task_number)]\ntrain_dataloaders = [DataLoader(data.dataset, batch_size=args.batch_size, shuffle=True) for data in train_datasets]\n\ntest_datasets = [Data('data', train=False, angle=angle_list[index]) for index in range(args.task_number)]\ntest_dataloaders = [DataLoader(data.dataset, batch_size=args.test_size, shuffle=True) for data in test_datasets]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization","metadata":{}},{"cell_type":"code","source":"# Visualize label 0-9 1 sample MNIST picture in 5 tasks.\nsample = [Data('data', angle=angle_list[index]) for index in range(args.task_number)]\n\nplt.figure(figsize=(30, 10))\nfor task in range(5):\n  target_list = []\n  cnt = 0\n  while (len(target_list) < 10):\n    img, target = sample[task].dataset[cnt]\n    cnt += 1\n    if target in target_list:\n      continue\n    else:\n      target_list.append(target)\n    plt.subplot(5, 10, (task)*10 + target + 1)\n    curr_img = np.reshape(img, (28, 28))\n    plt.matshow(curr_img, cmap=plt.get_cmap('gray'), fignum=False)\n    ax = plt.gca()\n    ax.axes.xaxis.set_ticks([])\n    ax.axes.yaxis.set_ticks([])\n    plt.title(\"task: \" + str(task+1) + \" \" + \"label: \" + str(target), y=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Model","metadata":{}},{"cell_type":"markdown","source":"### Model Architecture\nTo fair comparison, \n\nWe fix our model architecture to do this homework. \n\nThe model architecture consists of 4 layers fully-connected network.","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n  \"\"\"\n  Model architecture \n  1*28*28 (input) → 1024 → 512 → 256 → 10\n  \"\"\"\n  def __init__(self):\n    super(Model, self).__init__()\n    self.fc1 = nn.Linear(1*28*28, 1024)\n    self.fc2 = nn.Linear(1024, 512)\n    self.fc3 = nn.Linear(512, 256)\n    self.fc4 = nn.Linear(256, 10)\n    self.relu = nn.ReLU()\n\n  def forward(self, x):\n    x = x.view(-1, 1*28*28)\n    x = self.fc1(x)\n    x = self.relu(x)\n    x = self.fc2(x)\n    x = self.relu(x)\n    x = self.fc3(x)\n    x = self.relu(x)\n    x = self.fc4(x)\n    return x\n\nexample = Model()\nprint(example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Evaluate","metadata":{}},{"cell_type":"markdown","source":"### Train\nThis is our function of training.\n\nIt can generally be applied in different regularization-based lifelong learning algorithms in this homework.","metadata":{}},{"cell_type":"code","source":"def train(model, optimizer, dataloader, epochs_per_task, lll_object, lll_lambda, test_dataloaders, evaluate, device, log_step=1):\n  model.train()\n  model.zero_grad()\n  objective = nn.CrossEntropyLoss()\n  acc_per_epoch = []\n  loss = 1.0\n  bar = trange(epochs_per_task, leave=False, desc=f\"Epoch 1, Loss: {loss:.7f}\")\n  for epoch in bar:    \n    for imgs, labels in dataloader:   \n      imgs, labels = imgs.to(device), labels.to(device)\n      outputs = model(imgs)\n      loss = objective(outputs, labels)\n      total_loss = loss\n      lll_loss = lll_object.penalty(model)\n      total_loss += lll_lambda * lll_loss \n      lll_object.update(model)\n      optimizer.zero_grad()\n      total_loss.backward()\n      optimizer.step()\n\n      loss = total_loss.item()\n    acc_average  = []\n    for test_dataloader in test_dataloaders: \n      acc_test = evaluate(model, test_dataloader, device)\n      acc_average.append(acc_test)\n    average=np.mean(np.array(acc_average))\n    acc_per_epoch.append(average*100.0)\n                \n  return model, optimizer, acc_per_epoch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate\nThis is our function of evaluation.\n\nIt can generally be applied in different regularization-based lifelong learning algorithms in this homework.\n","metadata":{}},{"cell_type":"code","source":"def evaluate(model, test_dataloader, device):\n    model.eval()\n    correct_cnt = 0\n    total = 0\n    for imgs, labels in test_dataloader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        outputs = model(imgs)\n        _, pred_label = torch.max(outputs.data, 1)\n\n        correct_cnt += (pred_label == labels.data).sum().item()\n        total += torch.ones_like(labels.data).sum().item()\n    return correct_cnt / total","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Methods\n- Baseline\n- EWC\n- SI\n- MAS\n- RWalk\n- SCP","metadata":{}},{"cell_type":"markdown","source":"### Baseline\nThe baseline class will do nothing in the regularization term.","metadata":{}},{"cell_type":"code","source":"# Baseline\nclass baseline(object):\n  \"\"\"\n  baseline technique: do nothing in regularization term [initialize and all weight is zero]\n  \"\"\"\n  def __init__(self, model, dataloader, device):\n    self.model = model\n    self.dataloader = dataloader\n    self.device = device\n    # extract all parameters in models\n    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad} \n    \n    # store current parameters\n    self.p_old = {} \n\n    # generate weight matrix\n    self._precision_matrices = self._calculate_importance()  \n\n    for n, p in self.params.items():\n      # keep the old parameter in self.p_old\n      self.p_old[n] = p.clone().detach() \n\n  def _calculate_importance(self):\n    precision_matrices = {} \n    # initialize weight matrix（fill zero）\n    for n, p in self.params.items(): \n      precision_matrices[n] = p.clone().detach().fill_(0)\n\n    return precision_matrices\n\n  def penalty(self, model: nn.Module):\n    loss = 0\n    for n, p in model.named_parameters():\n      _loss = self._precision_matrices[n] * (p - self.p_old[n]) ** 2\n      loss += _loss.sum()\n    return loss\n  \n  def update(self, model):\n    # do nothing\n    return","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Baseline\nprint(\"RUN BASELINE\")\nmodel = Model()\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n# initialize lifelong learning object (baseline class) without adding any regularization term.\nlll_object=baseline(model=model, dataloader=None, device=device)\nlll_lambda=0.0\nbaseline_acc=[]\ntask_bar = trange(len(train_dataloaders),desc=\"Task   1\")\n\n# iterate training on each task continually.\nfor train_indexes in task_bar:\n  # Train each task\n  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, \n                  lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n  \n  # get model weight to baseline class and do nothing!\n  lll_object=baseline(model=model, dataloader=train_dataloaders[train_indexes],device=device)\n  \n  # new a optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n  \n  # Collect average accuracy in each epoch\n  baseline_acc.extend(acc_list)\n  \n  # display the information of the next task.\n  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n\n# average accuracy in each task per epoch! \nprint(baseline_acc)\nprint(\"==================================================================================================\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EWC\n\nElastic Weight Consolidation\n\nThe ewc class applied EWC algorithm to calculate the regularization term. The central concept is included in Prof.Hung-yi's lectures. Here we will focus on the algorithm of EWC.\n\nIn this assignment, we want to let our model learn 10 tasks successively. Here we show a simple example that lets the model learn 2 tasks(task A and task B) successively.\n\nIn the EWC algorithm, the definition of the loss function is shown below:\n $$\\mathcal{L}_B = \\mathcal{L}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_i (\\theta_{i} - \\theta_{A,i}^{*})^2  $$\n  \nAssume we have a neural network with more than two parameters.\n\n$F_i$ corresponds to the $i^{th}$ guard in Prof. Hung-yi's lecture. Please do not modify this parameter, because it's important to task A.\n\nThe definition of $F$ is shown below.\n$$ F = [ \\nabla \\log(p(y_n | x_n, \\theta_{A}^{*})) \\nabla \\log(p(y_n | x_n, \\theta_{A}^{*}))^T ] $$ \n\nWe only take the diagonal value of the matrix to approximate each parameter's $F_i$.\n\nThe detail information and derivation are shown in 2.4.1 and 2.4 of [Continual Learning in Neural\nNetworks](https://arxiv.org/pdf/1910.02718.pdf)\n\nFor You Information: [Elastic Weight Consolidation](https://arxiv.org/pdf/1612.00796.pdf)","metadata":{}},{"cell_type":"code","source":"# EWC\nclass ewc(object):\n  \"\"\"\n  @article{kirkpatrick2017overcoming,\n      title={Overcoming catastrophic forgetting in neural networks},\n      author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},\n      journal={Proceedings of the national academy of sciences},\n      year={2017},\n      url={https://arxiv.org/abs/1612.00796}\n  }\n  \"\"\"\n  def __init__(self, model, dataloader, device, prev_guards=[None]):\n    self.model = model\n    self.dataloader = dataloader\n    self.device = device\n    # extract all parameters in models\n    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad} \n    \n    # initialize parameters\n    self.p_old = {}\n    # save previous guards\n    self.previous_guards_list = prev_guards\n\n    # generate Fisher (F) matrix for EWC\n    self._precision_matrices = self._calculate_importance()                   \n\n    # keep the old parameter in self.p_old\n    for n, p in self.params.items():\n      self.p_old[n] = p.clone().detach()       \n\n  def _calculate_importance(self):\n    precision_matrices = {}\n    # initialize Fisher (F) matrix（all fill zero）and add previous guards\n    for n, p in self.params.items(): \n      precision_matrices[n] = p.clone().detach().fill_(0)                 \n      for i in range(len(self.previous_guards_list)):\n        if self.previous_guards_list[i]:\n          precision_matrices[n] += self.previous_guards_list[i][n]\n\n    self.model.eval()\n    if self.dataloader is not None:\n      number_data = len(self.dataloader)\n      for data in self.dataloader:\n        self.model.zero_grad()\n        # get image data\n        input = data[0].to(self.device)\n          \n        # image data forward model\n        output = self.model(input)\n          \n        # Simply use groud truth label of dataset.  \n        label = data[1].to(self.device)\n          \n        # generate Fisher(F) matrix for EWC     \n        loss = F.nll_loss(F.log_softmax(output, dim=1), label)\n        loss.backward()   \n\n        for n, p in self.model.named_parameters():\n          # get the gradient of each parameter and square it, then average it in all validation set.                          \n          precision_matrices[n].data += p.grad.data ** 2 / number_data   \n                                                                \n      precision_matrices = {n: p for n, p in precision_matrices.items()}\n    return precision_matrices\n\n  def penalty(self, model: nn.Module):\n    loss = 0\n    for n, p in model.named_parameters():\n      # generate the final regularization term by the ewc weight (self._precision_matrices[n]) and the square of weight difference ((p - self.p_old[n]) ** 2).  \n      _loss = self._precision_matrices[n] * (p - self.p_old[n]) ** 2\n      loss += _loss.sum()\n    return loss\n  \n  def update(self, model):\n    # do nothing\n    return ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EWC\nprint(\"RUN EWC\")\nmodel = Model()\nmodel = model.to(device)\n# initialize optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n# initialize lifelong learning object for EWC\nlll_object=ewc(model=model, dataloader=None, device=device)\n\n# setup the coefficient value of regularization term.\nlll_lambda=100\newc_acc= []\ntask_bar = trange(len(train_dataloaders),desc=\"Task   1\")\nprev_guards = []\n\n# iterate training on each task continually.\nfor train_indexes in task_bar:\n  # Train Each Task\n  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n  \n  # get model weight and calculate guidance for each weight\n  prev_guards.append(lll_object._precision_matrices)\n  lll_object=ewc(model=model, dataloader=train_dataloaders[train_indexes], device=device, prev_guards=prev_guards)\n\n  # new a Optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n  # collect average accuracy in each epoch\n  ewc_acc.extend(acc_list)\n\n  # Update tqdm displayer\n  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n\n# average accuracy in each task per epoch!     \nprint(ewc_acc)\nprint(\"==================================================================================================\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MAS\nMemory Aware Synapses\n\nThe mas class applied MAS algorithm to calculate the regularization term.\n\nThe concept of MAS is similar to EWC, the only difference is the calculation of the important weight. \nThe details are mentioned in the following blocks.\n\nMAS:\n\nIn MAS, the Loss function is shown below, the model learns task A before it learned task B.\n\n$$\\mathcal{L}_B = \\mathcal{L}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} \\Omega_i (\\theta_{i} - \\theta_{A,i}^{*})^2$$\n\nCompare with EWC, the $F_i$ in the loss function is replaced with $\\Omega_i$ in the following function.\n\n$$\\Omega_i = || \\frac{\\partial \\ell_2^2(M(x_k; \\theta))}{\\partial \\theta_i} || $$ \n\n$x_k$ is the sample data of the previous task. So the $\\Omega$ is obtained gradients of the squared L2-norm of the learned network output.\n\nThe method proposed in the paper is the local version by taking squared L2-norm outputs from each layer of the model.\n\nHere we only want you to implement the global version by taking outputs from the last layer of the model.\n\nFor Your Information: \n[Memory Aware Synapses](https://arxiv.org/pdf/1711.09601.pdf)","metadata":{}},{"cell_type":"code","source":"class mas(object):\n  \"\"\"\n  @article{aljundi2017memory,\n      title={Memory Aware Synapses: Learning what (not) to forget},\n      author={Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},\n      booktitle={ECCV},\n      year={2018},\n      url={https://eccv2018.org/openaccess/content_ECCV_2018/papers/Rahaf_Aljundi_Memory_Aware_Synapses_ECCV_2018_paper.pdf}\n  }\n  \"\"\"\n  def __init__(self, model: nn.Module, dataloader, device, prev_guards=[None]):\n    self.model = model \n    self.dataloader = dataloader\n    # extract all parameters in models\n    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad} \n    \n    # initialize parameters\n    self.p_old = {} \n    \n    self.device = device\n\n    # save previous guards\n    self.previous_guards_list = prev_guards\n    \n    # generate Omega(Ω) matrix for MAS\n    self._precision_matrices = self.calculate_importance() \n\n    # keep the old parameter in self.p_old\n    for n, p in self.params.items():\n      self.p_old[n] = p.clone().detach() \n  \n  def calculate_importance(self):\n    precision_matrices = {}\n    # initialize Omega(Ω) matrix（all filled zero）\n    for n, p in self.params.items():\n      precision_matrices[n] = p.clone().detach().fill_(0) \n      for i in range(len(self.previous_guards_list)):\n        if self.previous_guards_list[i]:\n          precision_matrices[n] += self.previous_guards_list[i][n]\n\n    self.model.eval()\n    if self.dataloader is not None:\n      num_data = len(self.dataloader)\n      for data in self.dataloader:\n        self.model.zero_grad()\n        output = self.model(data[0].to(self.device))\n        ################################################################\n        #####  TODO: generate Omega(Ω) matrix for MAS.  #####   \n        ################################################################\n        ################################################################                  \n    \n      precision_matrices = {n: p for n, p in precision_matrices.items()}\n    return precision_matrices\n\n  def penalty(self, model: nn.Module):\n    loss = 0\n    for n, p in model.named_parameters():\n      _loss = self._precision_matrices[n] * (p - self.p_old[n]) ** 2\n      loss += _loss.sum()\n    return loss\n\n  def update(self, model):\n    # do nothing\n    return ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAS\nprint(\"RUN MAS\")\nmodel = Model()\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\nlll_object=mas(model=model, dataloader=None, device=device)\nlll_lambda=0.1\nmas_acc= []\ntask_bar = trange(len(train_dataloaders),desc=\"Task   1\")\nprev_guards = []\n\nfor train_indexes in task_bar:\n  # Train Each Task\n  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n  \n  # get model weight and calculate guidance for each weight\n  prev_guards.append(lll_object._precision_matrices)\n  lll_object=mas(model=model, dataloader=train_dataloaders[train_indexes], device=device, prev_guards=prev_guards)\n\n  # New a Optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n  # Collect average accuracy in each epoch\n  mas_acc.extend(acc_list)\n  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n\n# average accuracy in each task per epoch!     \nprint(mas_acc)\nprint(\"==================================================================================================\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SI\nThe si class applied SI (Synaptic Intelligence) algorithm to calculate the regularization term.","metadata":{}},{"cell_type":"code","source":"# SI\nclass si(object):\n  \"\"\"\n  @article{kirkpatrick2017overcoming,\n      title={Overcoming catastrophic forgetting in neural networks},\n      author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},\n      journal={Proceedings of the national academy of sciences},\n      year={2017},\n      url={https://arxiv.org/abs/1612.00796}\n  }\n  \"\"\"\n  def __init__(self, model, dataloader, epsilon, device):\n    self.model = model\n    self.dataloader = dataloader\n    self.device = device\n    self.epsilon = epsilon\n    # extract all parameters in models\n    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n    \n    self._n_p_prev, self._n_omega = self._calculate_importance() \n    self.W, self.p_old = self._init_()\n    \n\n  def _init_(self):\n    W = {}\n    p_old = {}\n    for n, p in self.model.named_parameters():\n      n = n.replace('.', '__')\n      if p.requires_grad:\n        W[n] = p.data.clone().zero_()\n        p_old[n] = p.data.clone()\n    return W, p_old\n\n  def _calculate_importance(self):\n    n_p_prev = {}\n    n_omega = {}\n\n    if self.dataloader != None:\n      for n, p in self.model.named_parameters():\n        n = n.replace('.', '__')\n        if p.requires_grad:\n          # Find/calculate new values for quadratic penalty on parameters\n          p_prev = getattr(self.model, '{}_SI_prev_task'.format(n))\n          W = getattr(self.model, '{}_W'.format(n))\n          p_current = p.detach().clone()\n          p_change = p_current - p_prev\n          omega_add = W/(p_change**2 + self.epsilon)\n          try:\n            omega = getattr(self.model, '{}_SI_omega'.format(n))\n          except AttributeError:\n            omega = p.detach().clone().zero_()\n          omega_new = omega + omega_add\n          n_omega[n] = omega_new\n          n_p_prev[n] = p_current\n\n          # Store these new values in the model\n          self.model.register_buffer('{}_SI_prev_task'.format(n), p_current)\n          self.model.register_buffer('{}_SI_omega'.format(n), omega_new)\n\n    else:\n      for n, p in self.model.named_parameters():\n        n = n.replace('.', '__')\n        if p.requires_grad:\n          n_p_prev[n] = p.detach().clone()\n          n_omega[n] = p.detach().clone().zero_()\n          self.model.register_buffer('{}_SI_prev_task'.format(n), p.detach().clone())\n    return n_p_prev, n_omega\n\n  def penalty(self, model: nn.Module):\n    loss = 0.0\n    for n, p in model.named_parameters():\n      n = n.replace('.', '__')\n      if p.requires_grad:\n        prev_values = self._n_p_prev[n]\n        omega = self._n_omega[n]\n        _loss = omega * (p - prev_values) ** 2\n        loss += _loss.sum()\n    return loss\n  \n  def update(self, model):\n    for n, p in model.named_parameters():\n      n = n.replace('.', '__')\n      if p.requires_grad:\n        if p.grad is not None:\n          self.W[n].add_(-p.grad * (p.detach() - self.p_old[n]))\n          self.model.register_buffer('{}_W'.format(n), self.W[n])\n        self.p_old[n] = p.detach().clone()\n    return ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SI\nprint(\"RUN SI\")\nmodel = Model()\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\nlll_object=si(model=model, dataloader=None, epsilon=0.1, device=device)\nlll_lambda=1\nsi_acc = []\ntask_bar = trange(len(train_dataloaders),desc=\"Task   1\")\n\nfor train_indexes in task_bar:\n  # Train Each Task\n  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n  \n  # get model weight and calculate guidance for each weight\n  lll_object=si(model=model, dataloader=train_dataloaders[train_indexes], epsilon=0.1, device=device)\n\n  # New a Optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n  # Collect average accuracy in each epoch\n  si_acc.extend(acc_list)\n  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n\n# average accuracy in each task per epoch!     \nprint(si_acc)\nprint(\"==================================================================================================\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RWalk\n\n#### Remanian Walk for Incremental Learning\n\nThe rwalk class applied Remanian Walk algorithm to calculate the regularization term.\n\nThe details are mentioned in the following blocks.","metadata":{}},{"cell_type":"code","source":"class rwalk(object):\n  def __init__(self, model, dataloader, epsilon, device, prev_guards=[None]):\n    self.model = model\n    self.dataloader = dataloader\n    self.device = device\n    self.epsilon = epsilon\n    self.update_ewc_parameter = 0.4\n    # extract model parameters and store in dictionary\n    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n    \n    # initialize the guidance matrix\n    self._means = {} \n\n    self.previous_guards_list = prev_guards\n    \n    # Generate Fisher (F) Information Matrix\n    self._precision_matrices = self._calculate_importance_ewc()\n      \n    self._n_p_prev, self._n_omega = self._calculate_importance() \n    self.W, self.p_old = self._init_()\n\n  def _init_(self):\n    W = {}\n    p_old = {}\n    for n, p in self.model.named_parameters():\n      n = n.replace('.', '__')\n      if p.requires_grad:\n        W[n] = p.data.clone().zero_()\n        p_old[n] = p.data.clone()\n    return W, p_old\n\n  def _calculate_importance(self):\n    n_p_prev = {}\n    n_omega = {}\n\n    if self.dataloader is not None:\n      for n, p in self.model.named_parameters():\n        n = n.replace('.', '__')\n        if p.requires_grad:\n          # Find/calculate new values for quadratic penalty on parameters\n          p_prev = getattr(self.model, '{}_SI_prev_task'.format(n))\n          W = getattr(self.model, '{}_W'.format(n))\n          p_current = p.detach().clone()\n          p_change = p_current - p_prev\n          omega_add = W / (1.0 / 2.0*self._precision_matrices[n] *p_change**2 + self.epsilon)\n          try:\n              omega = getattr(self.model, '{}_SI_omega'.format(n))\n          except AttributeError:\n              omega = p.detach().clone().zero_()\n          omega_new = 0.5 * omega + 0.5 *omega_add\n          n_omega[n] = omega_new\n          n_p_prev[n] = p_current\n\n          # Store these new values in the model\n          self.model.register_buffer('{}_SI_prev_task'.format(n), p_current)\n          self.model.register_buffer('{}_SI_omega'.format(n), omega_new)\n\n    else:\n      for n, p in self.model.named_parameters():\n        n = n.replace('.', '__')\n        if p.requires_grad:\n          n_p_prev[n] = p.detach().clone()\n          n_omega[n] = p.detach().clone().zero_()\n          self.model.register_buffer('{}_SI_prev_task'.format(n), p.detach().clone())\n    return n_p_prev, n_omega\n  \n  def _calculate_importance_ewc(self):\n    precision_matrices = {}\n    for n, p in self.params.items():\n      # initialize Fisher (F) matrix（all fill zero） \n      n = n.replace('.', '__') \n      precision_matrices[n] = p.clone().detach().fill_(0)\n      for i in range(len(self.previous_guards_list)):\n        if self.previous_guards_list[i]:\n          precision_matrices[n] += self.previous_guards_list[i][n]\n           \n\n    self.model.eval()\n    if self.dataloader is not None:\n      number_data = len(self.dataloader)\n      for n, p in self.model.named_parameters():                         \n        n = n.replace('.', '__')\n        precision_matrices[n].data *= (1 - self.update_ewc_parameter)\n      for data in self.dataloader:\n        self.model.zero_grad()\n        input = data[0].to(self.device)\n        output = self.model(input)\n        label = data[1].to(self.device)\n\n        # generate Fisher(F) matrix for RWALK    \n        loss = F.nll_loss(F.log_softmax(output, dim=1), label) \n        loss.backward()                                                    \n                                                                          \n        for n, p in self.model.named_parameters():                         \n          n = n.replace('.', '__')\n          precision_matrices[n].data += self.update_ewc_parameter*p.grad.data ** 2 / number_data  \n                                                                  \n      precision_matrices = {n: p for n, p in precision_matrices.items()}\n    return precision_matrices\n\n  def penalty(self, model: nn.Module):\n    loss = 0.0\n    for n, p in model.named_parameters():\n      n = n.replace('.', '__')\n      if p.requires_grad:\n        prev_values = self._n_p_prev[n]\n        omega = self._n_omega[n]\n        # Generate regularization term  _loss by omega and Fisher Matrix\n        _loss = (omega + self._precision_matrices[n]) * (p - prev_values) ** 2\n        loss += _loss.sum()\n\n    return loss\n  \n  def update(self, model):\n    for n, p in model.named_parameters():\n      n = n.replace('.', '__')\n      if p.requires_grad:\n        if p.grad is not None:\n          self.W[n].add_(-p.grad * (p.detach() - self.p_old[n]))\n          self.model.register_buffer('{}_W'.format(n), self.W[n])\n        self.p_old[n] = p.detach().clone()\n    return ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RWalk\nprint(\"RUN Rwalk\")\nmodel = Model()\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\nlll_object=rwalk(model=model, dataloader=None, epsilon=0.1, device=device)\nlll_lambda=100\nrwalk_acc = []\ntask_bar = trange(len(train_dataloaders),desc=\"Task   1\")\nprev_guards = []\n\nfor train_indexes in task_bar:\n  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n  prev_guards.append(lll_object._precision_matrices)\n  lll_object=rwalk(model=model, dataloader=train_dataloaders[train_indexes], epsilon=0.1, device=device, prev_guards=prev_guards)\n  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n  rwalk_acc.extend(acc_list)\n  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n\n# average accuracy in each task per epoch!     \nprint(rwalk_acc)\nprint(\"==================================================================================================\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SCP\nSliced Cramer Preservation\n\nPseudo Code:\n<img src=\"https://i.ibb.co/QJycmNZ/2021-02-18-21-07.png\" width=\"100%\">","metadata":{}},{"cell_type":"code","source":"def sample_spherical(npoints, ndim=3):\n  vec = np.random.randn(ndim, npoints)\n  vec /= np.linalg.norm(vec, axis=0)\n  return torch.from_numpy(vec)\n\nclass scp(object):\n  \"\"\"\n  OPEN REVIEW VERSION:\n  https://openreview.net/forum?id=BJge3TNKwH\n  \"\"\"\n  def __init__(self, model: nn.Module, dataloader, L: int, device, prev_guards=[None]):\n    self.model = model \n    self.dataloader = dataloader\n    self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n    self._state_parameters = {}\n    self.L= L\n    self.device = device\n    self.previous_guards_list = prev_guards\n    self._precision_matrices = self.calculate_importance()\n    for n, p in self.params.items():\n      self._state_parameters[n] = p.clone().detach()\n  \n  def calculate_importance(self):\n    precision_matrices = {}\n    for n, p in self.params.items():\n      precision_matrices[n] = p.clone().detach().fill_(0)\n      for i in range(len(self.previous_guards_list)):\n        if self.previous_guards_list[i]:\n          precision_matrices[n] += self.previous_guards_list[i][n]\n\n    self.model.eval()\n    if self.dataloader is not None:\n      num_data = len(self.dataloader)\n      for data in self.dataloader:\n        self.model.zero_grad()\n        output = self.model(data[0].to(self.device))\n          \n        mean_vec = output.mean(dim=0)\n\n        L_vectors = sample_spherical(self.L, output.shape[-1])\n        L_vectors = L_vectors.transpose(1,0).to(self.device).float()\n                    \n        total_scalar = 0\n        for vec in L_vectors:\n          scalar=torch.matmul(vec, mean_vec)\n          total_scalar += scalar\n        total_scalar /= L_vectors.shape[0] \n        total_scalar.backward()     \n\n        for n, p in self.model.named_parameters():                      \n          precision_matrices[n].data += p.grad**2 / num_data      \n              \n    precision_matrices = {n: p for n, p in precision_matrices.items()}\n    return precision_matrices\n\n  def penalty(self, model: nn.Module):\n    loss = 0\n    for n, p in model.named_parameters():\n      _loss = self._precision_matrices[n] * (p - self._state_parameters[n]) ** 2\n      loss += _loss.sum()\n    return loss\n  \n  def update(self, model):\n    # do nothing\n    return ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SCP\nprint(\"RUN SLICE CRAMER PRESERVATION\")\nmodel = Model()\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\nlll_object=scp(model=model, dataloader=None, L=100, device=device)\nlll_lambda=100\nscp_acc= []\ntask_bar = trange(len(train_dataloaders),desc=\"Task   1\")\nprev_guards = []\n\nfor train_indexes in task_bar:\n  model, _, acc_list = train(model, optimizer, train_dataloaders[train_indexes], args.epochs_per_task, lll_object, lll_lambda, evaluate=evaluate,device=device, test_dataloaders=test_dataloaders[:train_indexes+1])\n  prev_guards.append(lll_object._precision_matrices)\n  lll_object=scp(model=model, dataloader=train_dataloaders[train_indexes], L=100, device=device, prev_guards=prev_guards)\n  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n  scp_acc.extend(acc_list)\n  task_bar.set_description_str(f\"Task  {train_indexes+2:2}\")\n\n# average accuracy in each task per epoch!     \nprint(scp_acc)\nprint(\"==================================================================================================\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot function","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef draw_acc(acc_list, label_list):\n  for acc, label in zip(acc_list, label_list):\n    plt.plot(acc, marker='o', ls='--', lw=2, markersize=4, label=label)\n    plt.legend()\n  plt.savefig('acc_summary.png')\n  plt.show() \n\nacc_list = [baseline_acc, ewc_acc, mas_acc, si_acc, rwalk_acc, scp_acc]\nlabel_list = ['baseline', 'EWC', 'MAS', 'SI', 'RWALK', 'SCP']\ndraw_acc(acc_list, label_list)","metadata":{},"execution_count":null,"outputs":[]}]}